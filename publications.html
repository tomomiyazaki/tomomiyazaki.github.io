<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title></title>
  <style>
  @font-face {
  font-family: octicons-link;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body .octicon {
  display: inline-block;
  fill: currentColor;
  vertical-align: text-bottom;
}

.markdown-body .anchor {
  float: left;
  line-height: 1;
  margin-left: -20px;
  padding-right: 4px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #1b1f23;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #24292e;
  line-height: 1.5;
  font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;
  font-size: 16px;
  line-height: 1.5;
  word-wrap: break-word;
}

.markdown-body .pl-c {
  color: #6a737d;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #005cc5;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #6f42c1;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #24292e;
}

.markdown-body .pl-ent {
  color: #22863a;
}

.markdown-body .pl-k {
  color: #d73a49;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #032f62;
}

.markdown-body .pl-smw,
.markdown-body .pl-v {
  color: #e36209;
}

.markdown-body .pl-bu {
  color: #b31d28;
}

.markdown-body .pl-ii {
  background-color: #b31d28;
  color: #fafbfc;
}

.markdown-body .pl-c2 {
  background-color: #d73a49;
  color: #fafbfc;
}

.markdown-body .pl-c2:before {
  content: "^M";
}

.markdown-body .pl-sr .pl-cce {
  color: #22863a;
  font-weight: 700;
}

.markdown-body .pl-ml {
  color: #735c0f;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #005cc5;
  font-weight: 700;
}

.markdown-body .pl-mi {
  color: #24292e;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #24292e;
  font-weight: 700;
}

.markdown-body .pl-md {
  background-color: #ffeef0;
  color: #b31d28;
}

.markdown-body .pl-mi1 {
  background-color: #f0fff4;
  color: #22863a;
}

.markdown-body .pl-mc {
  background-color: #ffebda;
  color: #e36209;
}

.markdown-body .pl-mi2 {
  background-color: #005cc5;
  color: #f6f8fa;
}

.markdown-body .pl-mdr {
  color: #6f42c1;
  font-weight: 700;
}

.markdown-body .pl-ba {
  color: #586069;
}

.markdown-body .pl-sg {
  color: #959da5;
}

.markdown-body .pl-corl {
  color: #032f62;
  text-decoration: underline;
}

.markdown-body details {
  display: block;
}

.markdown-body summary {
  display: list-item;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline-width: 0;
}

.markdown-body strong {
  font-weight: inherit;
  font-weight: bolder;
}

.markdown-body h1 {
  font-size: 2em;
  margin: .67em 0;
}

.markdown-body img {
  border-style: none;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace,monospace;
  font-size: 1em;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
  overflow: visible;
}

.markdown-body input {
  font: inherit;
  margin: 0;
}

.markdown-body input {
  overflow: visible;
}

.markdown-body [type=checkbox] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body * {
  box-sizing: border-box;
}

.markdown-body input {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}

.markdown-body a {
  color: #0366d6;
  text-decoration: none;
}

.markdown-body a:hover {
  text-decoration: underline;
}

.markdown-body strong {
  font-weight: 600;
}

.markdown-body hr {
  background: transparent;
  border: 0;
  border-bottom: 1px solid #dfe2e5;
  height: 0;
  margin: 15px 0;
  overflow: hidden;
}

.markdown-body hr:before {
  content: "";
  display: table;
}

.markdown-body hr:after {
  clear: both;
  content: "";
  display: table;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body details summary {
  cursor: pointer;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body h1 {
  font-size: 32px;
}

.markdown-body h1,
.markdown-body h2 {
  font-weight: 600;
}

.markdown-body h2 {
  font-size: 24px;
}

.markdown-body h3 {
  font-size: 20px;
}

.markdown-body h3,
.markdown-body h4 {
  font-weight: 600;
}

.markdown-body h4 {
  font-size: 16px;
}

.markdown-body h5 {
  font-size: 14px;
}

.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
}

.markdown-body h6 {
  font-size: 12px;
}

.markdown-body p {
  margin-bottom: 10px;
  margin-top: 0;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ol,
.markdown-body ul {
  margin-bottom: 0;
  margin-top: 0;
  padding-left: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ol ol ol,
.markdown-body ol ul ol,
.markdown-body ul ol ol,
.markdown-body ul ul ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre {
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body input::-webkit-inner-spin-button,
.markdown-body input::-webkit-outer-spin-button {
  -webkit-appearance: none;
  appearance: none;
  margin: 0;
}

.markdown-body .border {
  border: 1px solid #e1e4e8!important;
}

.markdown-body .border-0 {
  border: 0!important;
}

.markdown-body .border-bottom {
  border-bottom: 1px solid #e1e4e8!important;
}

.markdown-body .rounded-1 {
  border-radius: 3px!important;
}

.markdown-body .bg-white {
  background-color: #fff!important;
}

.markdown-body .bg-gray-light {
  background-color: #fafbfc!important;
}

.markdown-body .text-gray-light {
  color: #6a737d!important;
}

.markdown-body .mb-0 {
  margin-bottom: 0!important;
}

.markdown-body .my-2 {
  margin-bottom: 8px!important;
  margin-top: 8px!important;
}

.markdown-body .pl-0 {
  padding-left: 0!important;
}

.markdown-body .py-0 {
  padding-bottom: 0!important;
  padding-top: 0!important;
}

.markdown-body .pl-1 {
  padding-left: 4px!important;
}

.markdown-body .pl-2 {
  padding-left: 8px!important;
}

.markdown-body .py-2 {
  padding-bottom: 8px!important;
  padding-top: 8px!important;
}

.markdown-body .pl-3,
.markdown-body .px-3 {
  padding-left: 16px!important;
}

.markdown-body .px-3 {
  padding-right: 16px!important;
}

.markdown-body .pl-4 {
  padding-left: 24px!important;
}

.markdown-body .pl-5 {
  padding-left: 32px!important;
}

.markdown-body .pl-6 {
  padding-left: 40px!important;
}

.markdown-body .f6 {
  font-size: 12px!important;
}

.markdown-body .lh-condensed {
  line-height: 1.25!important;
}

.markdown-body .text-bold {
  font-weight: 600!important;
}

.markdown-body:before {
  content: "";
  display: table;
}

.markdown-body:after {
  clear: both;
  content: "";
  display: table;
}

.markdown-body>:first-child {
  margin-top: 0!important;
}

.markdown-body>:last-child {
  margin-bottom: 0!important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body blockquote,
.markdown-body dl,
.markdown-body ol,
.markdown-body p,
.markdown-body pre,
.markdown-body table,
.markdown-body ul {
  margin-bottom: 16px;
  margin-top: 0;
}

.markdown-body hr {
  background-color: #e1e4e8;
  border: 0;
  height: .25em;
  margin: 24px 0;
  padding: 0;
}

.markdown-body blockquote {
  border-left: .25em solid #dfe2e5;
  color: #6a737d;
  padding: 0 1em;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #fafbfc;
  border: 1px solid #c6cbd1;
  border-bottom-color: #959da5;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #959da5;
  color: #444d56;
  display: inline-block;
  font-size: 11px;
  line-height: 10px;
  padding: 3px 5px;
  vertical-align: middle;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  font-weight: 600;
  line-height: 1.25;
  margin-bottom: 16px;
  margin-top: 24px;
}

.markdown-body h1 {
  font-size: 2em;
}

.markdown-body h1,
.markdown-body h2 {
  border-bottom: 1px solid #eaecef;
  padding-bottom: .3em;
}

.markdown-body h2 {
  font-size: 1.5em;
}

.markdown-body h3 {
  font-size: 1.25em;
}

.markdown-body h4 {
  font-size: 1em;
}

.markdown-body h5 {
  font-size: .875em;
}

.markdown-body h6 {
  color: #6a737d;
  font-size: .85em;
}

.markdown-body ol,
.markdown-body ul {
  padding-left: 2em;
}

.markdown-body ol ol,
.markdown-body ol ul,
.markdown-body ul ol,
.markdown-body ul ul {
  margin-bottom: 0;
  margin-top: 0;
}

.markdown-body li {
  word-wrap: break-all;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body li+li {
  margin-top: .25em;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  font-size: 1em;
  font-style: italic;
  font-weight: 600;
  margin-top: 16px;
  padding: 0;
}

.markdown-body dl dd {
  margin-bottom: 16px;
  padding: 0 16px;
}

.markdown-body table {
  display: block;
  overflow: auto;
  width: 100%;
}

.markdown-body table th {
  font-weight: 600;
}

.markdown-body table td,
.markdown-body table th {
  border: 1px solid #dfe2e5;
  padding: 6px 13px;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f6f8fa;
}

.markdown-body img {
  background-color: #fff;
  box-sizing: content-box;
  max-width: 100%;
}

.markdown-body img[align=right] {
  padding-left: 20px;
}

.markdown-body img[align=left] {
  padding-right: 20px;
}

.markdown-body code {
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
  font-size: 85%;
  margin: 0;
  padding: .2em .4em;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre>code {
  background: transparent;
  border: 0;
  font-size: 100%;
  margin: 0;
  padding: 0;
  white-space: pre;
  word-break: normal;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body .highlight pre,
.markdown-body pre {
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
  line-height: 1.45;
  overflow: auto;
  padding: 16px;
}

.markdown-body pre code {
  background-color: transparent;
  border: 0;
  display: inline;
  line-height: inherit;
  margin: 0;
  max-width: auto;
  overflow: visible;
  padding: 0;
  word-wrap: normal;
}

.markdown-body .commit-tease-sha {
  color: #444d56;
  display: inline-block;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 90%;
}

.markdown-body .blob-wrapper {
  border-bottom-left-radius: 3px;
  border-bottom-right-radius: 3px;
  overflow-x: auto;
  overflow-y: hidden;
}

.markdown-body .blob-wrapper-embedded {
  max-height: 240px;
  overflow-y: auto;
}

.markdown-body .blob-num {
  -moz-user-select: none;
  -ms-user-select: none;
  -webkit-user-select: none;
  color: rgba(27,31,35,.3);
  cursor: pointer;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
  line-height: 20px;
  min-width: 50px;
  padding-left: 10px;
  padding-right: 10px;
  text-align: right;
  user-select: none;
  vertical-align: top;
  white-space: nowrap;
  width: 1%;
}

.markdown-body .blob-num:hover {
  color: rgba(27,31,35,.6);
}

.markdown-body .blob-num:before {
  content: attr(data-line-number);
}

.markdown-body .blob-code {
  line-height: 20px;
  padding-left: 10px;
  padding-right: 10px;
  position: relative;
  vertical-align: top;
}

.markdown-body .blob-code-inner {
  color: #24292e;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  font-size: 12px;
  overflow: visible;
  white-space: pre;
  word-wrap: normal;
}

.markdown-body .pl-token.active,
.markdown-body .pl-token:hover {
  background: #ffea7f;
  cursor: pointer;
}

.markdown-body kbd {
  background-color: #fafbfc;
  border: 1px solid #d1d5da;
  border-bottom-color: #c6cbd1;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #c6cbd1;
  color: #444d56;
  display: inline-block;
  font: 11px SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
  line-height: 10px;
  padding: 3px 5px;
  vertical-align: middle;
}

.markdown-body :checked+.radio-label {
  border-color: #0366d6;
  position: relative;
  z-index: 1;
}

.markdown-body .tab-size[data-tab-size="1"] {
  -moz-tab-size: 1;
  tab-size: 1;
}

.markdown-body .tab-size[data-tab-size="2"] {
  -moz-tab-size: 2;
  tab-size: 2;
}

.markdown-body .tab-size[data-tab-size="3"] {
  -moz-tab-size: 3;
  tab-size: 3;
}

.markdown-body .tab-size[data-tab-size="4"] {
  -moz-tab-size: 4;
  tab-size: 4;
}

.markdown-body .tab-size[data-tab-size="5"] {
  -moz-tab-size: 5;
  tab-size: 5;
}

.markdown-body .tab-size[data-tab-size="6"] {
  -moz-tab-size: 6;
  tab-size: 6;
}

.markdown-body .tab-size[data-tab-size="7"] {
  -moz-tab-size: 7;
  tab-size: 7;
}

.markdown-body .tab-size[data-tab-size="8"] {
  -moz-tab-size: 8;
  tab-size: 8;
}

.markdown-body .tab-size[data-tab-size="9"] {
  -moz-tab-size: 9;
  tab-size: 9;
}

.markdown-body .tab-size[data-tab-size="10"] {
  -moz-tab-size: 10;
  tab-size: 10;
}

.markdown-body .tab-size[data-tab-size="11"] {
  -moz-tab-size: 11;
  tab-size: 11;
}

.markdown-body .tab-size[data-tab-size="12"] {
  -moz-tab-size: 12;
  tab-size: 12;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 .2em .25em -1.6em;
  vertical-align: middle;
}

.markdown-body hr {
  border-bottom-color: #eee;
}

.markdown-body .pl-0 {
  padding-left: 0!important;
}

.markdown-body .pl-1 {
  padding-left: 4px!important;
}

.markdown-body .pl-2 {
  padding-left: 8px!important;
}

.markdown-body .pl-3 {
  padding-left: 16px!important;
}

.markdown-body .pl-4 {
  padding-left: 24px!important;
}

.markdown-body .pl-5 {
  padding-left: 32px!important;
}

.markdown-body .pl-6 {
  padding-left: 40px!important;
}

.markdown-body .pl-7 {
  padding-left: 48px!important;
}

.markdown-body .pl-8 {
  padding-left: 64px!important;
}

.markdown-body .pl-9 {
  padding-left: 80px!important;
}

.markdown-body .pl-10 {
  padding-left: 96px!important;
}

.markdown-body .pl-11 {
  padding-left: 112px!important;
}

.markdown-body .pl-12 {
  padding-left: 128px!important;
}

/*# sourceURL=webpack://./node_modules/github-markdown-css/github-markdown.css */
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,{"version":3,"sources":["webpack://./node_modules/github-markdown-css/github-markdown.css"],"names":[],"mappings":"AAAA;EACE,0BAA0B;EAC1B,2DAAqtE;AACvtE;;AAEA;EACE,qBAAqB;EACrB,kBAAkB;EAClB,2BAA2B;AAC7B;;AAEA;EACE,WAAW;EACX,cAAc;EACd,kBAAkB;EAClB,kBAAkB;AACpB;;AAEA;EACE,aAAa;AACf;;AAEA;;;;;;EAME,cAAc;EACd,sBAAsB;EACtB,kBAAkB;AACpB;;AAEA;;;;;;EAME,qBAAqB;AACvB;;AAEA;;;;;;EAME,mBAAmB;AACrB;;AAEA;EACE,0BAA0B;EAC1B,8BAA8B;EAC9B,cAAc;EACd,gBAAgB;EAChB,kIAAkI;EAClI,eAAe;EACf,gBAAgB;EAChB,qBAAqB;AACvB;;AAEA;EACE,cAAc;AAChB;;AAEA;;EAEE,cAAc;AAChB;;AAEA;;EAEE,cAAc;AAChB;;AAEA;;EAEE,cAAc;AAChB;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,cAAc;AAChB;;AAEA;;;;;;;EAOE,cAAc;AAChB;;AAEA;;EAEE,cAAc;AAChB;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,aAAa;AACf;;AAEA;EACE,cAAc;EACd,gBAAgB;AAClB;;AAEA;EACE,cAAc;AAChB;;AAEA;;;EAGE,cAAc;EACd,gBAAgB;AAClB;;AAEA;EACE,cAAc;EACd,kBAAkB;AACpB;;AAEA;EACE,cAAc;EACd,gBAAgB;AAClB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,cAAc;AAChB;;AAEA;EACE,cAAc;EACd,gBAAgB;AAClB;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,cAAc;EACd,0BAA0B;AAC5B;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,kBAAkB;AACpB;;AAEA;EACE,6BAA6B;AAC/B;;AAEA;;EAEE,gBAAgB;AAClB;;AAEA;EACE,oBAAoB;EACpB,mBAAmB;AACrB;;AAEA;EACE,cAAc;EACd,eAAe;AACjB;;AAEA;EACE,kBAAkB;AACpB;;AAEA;;;EAGE,gCAAgC;EAChC,cAAc;AAChB;;AAEA;EACE,uBAAuB;EACvB,SAAS;EACT,iBAAiB;AACnB;;AAEA;EACE,aAAa;EACb,SAAS;AACX;;AAEA;EACE,iBAAiB;AACnB;;AAEA;EACE,sBAAsB;EACtB,UAAU;AACZ;;AAEA;EACE,sBAAsB;AACxB;;AAEA;EACE,oBAAoB;EACpB,kBAAkB;EAClB,oBAAoB;AACtB;;AAEA;EACE,cAAc;EACd,qBAAqB;AACvB;;AAEA;EACE,0BAA0B;AAC5B;;AAEA;EACE,gBAAgB;AAClB;;AAEA;EACE,uBAAuB;EACvB,SAAS;EACT,gCAAgC;EAChC,SAAS;EACT,cAAc;EACd,gBAAgB;AAClB;;AAEA;EACE,WAAW;EACX,cAAc;AAChB;;AAEA;EACE,WAAW;EACX,WAAW;EACX,cAAc;AAChB;;AAEA;EACE,yBAAyB;EACzB,iBAAiB;AACnB;;AAEA;;EAEE,UAAU;AACZ;;AAEA;EACE,eAAe;AACjB;;AAEA;;;;;;EAME,gBAAgB;EAChB,aAAa;AACf;;AAEA;EACE,eAAe;AACjB;;AAEA;;EAEE,gBAAgB;AAClB;;AAEA;EACE,eAAe;AACjB;;AAEA;EACE,eAAe;AACjB;;AAEA;;EAEE,gBAAgB;AAClB;;AAEA;EACE,eAAe;AACjB;;AAEA;EACE,eAAe;AACjB;;AAEA;;EAEE,gBAAgB;AAClB;;AAEA;EACE,eAAe;AACjB;;AAEA;EACE,mBAAmB;EACnB,aAAa;AACf;;AAEA;EACE,SAAS;AACX;;AAEA;;EAEE,gBAAgB;EAChB,aAAa;EACb,eAAe;AACjB;;AAEA;;EAEE,4BAA4B;AAC9B;;AAEA;;;;EAIE,4BAA4B;AAC9B;;AAEA;EACE,cAAc;AAChB;;AAEA;;EAEE,4EAA4E;EAC5E,eAAe;AACjB;;AAEA;EACE,gBAAgB;EAChB,aAAa;AACf;;AAEA;;EAEE,wBAAwB;EACxB,gBAAgB;EAChB,SAAS;AACX;;AAEA;EACE,mCAAmC;AACrC;;AAEA;EACE,mBAAmB;AACrB;;AAEA;EACE,0CAA0C;AAC5C;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,gCAAgC;AAClC;;AAEA;EACE,mCAAmC;AACrC;;AAEA;EACE,wBAAwB;AAC1B;;AAEA;EACE,0BAA0B;AAC5B;;AAEA;EACE,4BAA4B;EAC5B,yBAAyB;AAC3B;;AAEA;EACE,yBAAyB;AAC3B;;AAEA;EACE,2BAA2B;EAC3B,wBAAwB;AAC1B;;AAEA;EACE,2BAA2B;AAC7B;;AAEA;EACE,2BAA2B;AAC7B;;AAEA;EACE,6BAA6B;EAC7B,0BAA0B;AAC5B;;AAEA;;EAEE,4BAA4B;AAC9B;;AAEA;EACE,6BAA6B;AAC/B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,yBAAyB;AAC3B;;AAEA;EACE,2BAA2B;AAC7B;;AAEA;EACE,0BAA0B;AAC5B;;AAEA;EACE,WAAW;EACX,cAAc;AAChB;;AAEA;EACE,WAAW;EACX,WAAW;EACX,cAAc;AAChB;;AAEA;EACE,uBAAuB;AACzB;;AAEA;EACE,0BAA0B;AAC5B;;AAEA;EACE,cAAc;EACd,qBAAqB;AACvB;;AAEA;;;;;;;EAOE,mBAAmB;EACnB,aAAa;AACf;;AAEA;EACE,yBAAyB;EACzB,SAAS;EACT,aAAa;EACb,cAAc;EACd,UAAU;AACZ;;AAEA;EACE,gCAAgC;EAChC,cAAc;EACd,cAAc;AAChB;;AAEA;EACE,aAAa;AACf;;AAEA;EACE,gBAAgB;AAClB;;AAEA;EACE,yBAAyB;EACzB,yBAAyB;EACzB,4BAA4B;EAC5B,kBAAkB;EAClB,kCAAkC;EAClC,cAAc;EACd,qBAAqB;EACrB,eAAe;EACf,iBAAiB;EACjB,gBAAgB;EAChB,sBAAsB;AACxB;;AAEA;;;;;;EAME,gBAAgB;EAChB,iBAAiB;EACjB,mBAAmB;EACnB,gBAAgB;AAClB;;AAEA;EACE,cAAc;AAChB;;AAEA;;EAEE,gCAAgC;EAChC,oBAAoB;AACtB;;AAEA;EACE,gBAAgB;AAClB;;AAEA;EACE,iBAAiB;AACnB;;AAEA;EACE,cAAc;AAChB;;AAEA;EACE,iBAAiB;AACnB;;AAEA;EACE,cAAc;EACd,gBAAgB;AAClB;;AAEA;;EAEE,iBAAiB;AACnB;;AAEA;;;;EAIE,gBAAgB;EAChB,aAAa;AACf;;AAEA;EACE,oBAAoB;AACtB;;AAEA;EACE,gBAAgB;AAClB;;AAEA;EACE,iBAAiB;AACnB;;AAEA;EACE,UAAU;AACZ;;AAEA;EACE,cAAc;EACd,kBAAkB;EAClB,gBAAgB;EAChB,gBAAgB;EAChB,UAAU;AACZ;;AAEA;EACE,mBAAmB;EACnB,eAAe;AACjB;;AAEA;EACE,cAAc;EACd,cAAc;EACd,WAAW;AACb;;AAEA;EACE,gBAAgB;AAClB;;AAEA;;EAEE,yBAAyB;EACzB,iBAAiB;AACnB;;AAEA;EACE,sBAAsB;EACtB,6BAA6B;AAC/B;;AAEA;EACE,yBAAyB;AAC3B;;AAEA;EACE,sBAAsB;EACtB,uBAAuB;EACvB,eAAe;AACjB;;AAEA;EACE,kBAAkB;AACpB;;AAEA;EACE,mBAAmB;AACrB;;AAEA;EACE,oCAAoC;EACpC,kBAAkB;EAClB,cAAc;EACd,SAAS;EACT,kBAAkB;AACpB;;AAEA;EACE,iBAAiB;AACnB;;AAEA;EACE,uBAAuB;EACvB,SAAS;EACT,eAAe;EACf,SAAS;EACT,UAAU;EACV,gBAAgB;EAChB,kBAAkB;AACpB;;AAEA;EACE,mBAAmB;AACrB;;AAEA;EACE,gBAAgB;EAChB,kBAAkB;AACpB;;AAEA;;EAEE,yBAAyB;EACzB,kBAAkB;EAClB,cAAc;EACd,iBAAiB;EACjB,cAAc;EACd,aAAa;AACf;;AAEA;EACE,6BAA6B;EAC7B,SAAS;EACT,eAAe;EACf,oBAAoB;EACpB,SAAS;EACT,eAAe;EACf,iBAAiB;EACjB,UAAU;EACV,iBAAiB;AACnB;;AAEA;EACE,cAAc;EACd,qBAAqB;EACrB,4EAA4E;EAC5E,cAAc;AAChB;;AAEA;EACE,8BAA8B;EAC9B,+BAA+B;EAC/B,gBAAgB;EAChB,kBAAkB;AACpB;;AAEA;EACE,iBAAiB;EACjB,gBAAgB;AAClB;;AAEA;EACE,sBAAsB;EACtB,qBAAqB;EACrB,yBAAyB;EACzB,wBAAwB;EACxB,eAAe;EACf,4EAA4E;EAC5E,eAAe;EACf,iBAAiB;EACjB,eAAe;EACf,kBAAkB;EAClB,mBAAmB;EACnB,iBAAiB;EACjB,iBAAiB;EACjB,mBAAmB;EACnB,mBAAmB;EACnB,SAAS;AACX;;AAEA;EACE,wBAAwB;AAC1B;;AAEA;EACE,+BAA+B;AACjC;;AAEA;EACE,iBAAiB;EACjB,kBAAkB;EAClB,mBAAmB;EACnB,kBAAkB;EAClB,mBAAmB;AACrB;;AAEA;EACE,cAAc;EACd,4EAA4E;EAC5E,eAAe;EACf,iBAAiB;EACjB,gBAAgB;EAChB,iBAAiB;AACnB;;AAEA;;EAEE,mBAAmB;EACnB,eAAe;AACjB;;AAEA;EACE,yBAAyB;EACzB,yBAAyB;EACzB,4BAA4B;EAC5B,kBAAkB;EAClB,kCAAkC;EAClC,cAAc;EACd,qBAAqB;EACrB,0EAA0E;EAC1E,iBAAiB;EACjB,gBAAgB;EAChB,sBAAsB;AACxB;;AAEA;EACE,qBAAqB;EACrB,kBAAkB;EAClB,UAAU;AACZ;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,gBAAgB;EAChB,WAAW;AACb;;AAEA;EACE,iBAAiB;EACjB,YAAY;AACd;;AAEA;EACE,iBAAiB;EACjB,YAAY;AACd;;AAEA;EACE,iBAAiB;EACjB,YAAY;AACd;;AAEA;EACE,qBAAqB;AACvB;;AAEA;EACE,eAAe;AACjB;;AAEA;EACE,2BAA2B;EAC3B,sBAAsB;AACxB;;AAEA;EACE,yBAAyB;AAC3B;;AAEA;EACE,yBAAyB;AAC3B;;AAEA;EACE,2BAA2B;AAC7B;;AAEA;EACE,2BAA2B;AAC7B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,4BAA4B;AAC9B;;AAEA;EACE,6BAA6B;AAC/B;;AAEA;EACE,6BAA6B;AAC/B","sourceRoot":""} */
  </style>
  <style>
  /**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #9a6e3a;
	/* This background color was intended by the author of this theme. */
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function,
.token.class-name {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}

/*# sourceURL=webpack://./node_modules/prismjs/themes/prism.css */
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJzb3VyY2VzIjpbIndlYnBhY2s6Ly8uL25vZGVfbW9kdWxlcy9wcmlzbWpzL3RoZW1lcy9wcmlzbS5jc3MiXSwibmFtZXMiOltdLCJtYXBwaW5ncyI6IkFBQUE7Ozs7RUFJRTs7QUFFRjs7Q0FFQyxZQUFZO0NBQ1osZ0JBQWdCO0NBQ2hCLHdCQUF3QjtDQUN4QixzRUFBc0U7Q0FDdEUsY0FBYztDQUNkLGdCQUFnQjtDQUNoQixnQkFBZ0I7Q0FDaEIsb0JBQW9CO0NBQ3BCLGtCQUFrQjtDQUNsQixpQkFBaUI7Q0FDakIsZ0JBQWdCOztDQUVoQixnQkFBZ0I7Q0FDaEIsY0FBYztDQUNkLFdBQVc7O0NBRVgscUJBQXFCO0NBQ3JCLGtCQUFrQjtDQUNsQixpQkFBaUI7Q0FDakIsYUFBYTtBQUNkOztBQUVBOztDQUVDLGlCQUFpQjtDQUNqQixtQkFBbUI7QUFDcEI7O0FBRUE7O0NBRUMsaUJBQWlCO0NBQ2pCLG1CQUFtQjtBQUNwQjs7QUFFQTtDQUNDOztFQUVDLGlCQUFpQjtDQUNsQjtBQUNEOztBQUVBLGdCQUFnQjtBQUNoQjtDQUNDLFlBQVk7Q0FDWixjQUFjO0NBQ2QsY0FBYztBQUNmOztBQUVBOztDQUVDLG1CQUFtQjtBQUNwQjs7QUFFQSxnQkFBZ0I7QUFDaEI7Q0FDQyxhQUFhO0NBQ2IsbUJBQW1CO0NBQ25CLG1CQUFtQjtBQUNwQjs7QUFFQTs7OztDQUlDLGdCQUFnQjtBQUNqQjs7QUFFQTtDQUNDLFdBQVc7QUFDWjs7QUFFQTtDQUNDLFdBQVc7QUFDWjs7QUFFQTs7Ozs7OztDQU9DLFdBQVc7QUFDWjs7QUFFQTs7Ozs7O0NBTUMsV0FBVztBQUNaOztBQUVBOzs7OztDQUtDLGNBQWM7Q0FDZCxvRUFBb0U7Q0FDcEUsaUNBQWlDO0FBQ2xDOztBQUVBOzs7Q0FHQyxXQUFXO0FBQ1o7O0FBRUE7O0NBRUMsY0FBYztBQUNmOztBQUVBOzs7Q0FHQyxXQUFXO0FBQ1o7O0FBRUE7O0NBRUMsaUJBQWlCO0FBQ2xCO0FBQ0E7Q0FDQyxrQkFBa0I7QUFDbkI7O0FBRUE7Q0FDQyxZQUFZO0FBQ2IiLCJzb3VyY2VSb290IjoiIn0= */
  </style>
  <style>
  
  </style>
  <style>
    .markdown-body {
      font-family: -apple-system,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;
      box-sizing: border-box;
      min-width: 200px;
      max-width: 980px;
      margin: 0 auto;
      padding: 45px;
    }

    @media not print {
      .markdown-body {
        padding: 45px;
      }

      @media (max-width: 767px) {
        .markdown-body {
          padding: 15px;
        }
      }
    }

    .hf-container {
      color: #24292e;
      line-height: 1.3;
    }

    .markdown-body .highlight pre,
    .markdown-body pre {
      white-space: pre-wrap;
    }
    .markdown-body table {
      display: table;
    }
    .markdown-body img[data-align="center"] {
      display: block;
      margin: 0 auto;
    }
    .markdown-body img[data-align="right"] {
      display: block;
      margin: 0 0 0 auto;
    }
    .markdown-body li.task-list-item {
      list-style-type: none;
    }
    .markdown-body li > [type=checkbox] {
      margin: 0 0 0 -1.3em;
    }
    .markdown-body input[type="checkbox"] ~ p {
      margin-top: 0;
      display: inline-block;
    }
    .markdown-body ol ol,
    .markdown-body ul ol {
      list-style-type: decimal;
    }
    .markdown-body ol ol ol,
    .markdown-body ol ul ol,
    .markdown-body ul ol ol,
    .markdown-body ul ul ol {
      list-style-type: decimal;
    }
  </style>
  <style>.markdown-body a.footnote-ref {
  text-decoration: none;
}

.footnotes {
  font-size: .85em;
  opacity: .8;
}

.footnotes li[role="doc-endnote"] {
  position: relative;
}

.footnotes .footnote-back {
  position: absolute;
  font-family: initial;
  top: .2em;
  right: 1em;
  text-decoration: none;
}

.inline-math.invalid,
.multiple-math.invalid {
  color: rgb(255, 105, 105);
}

.toc-container {
  width: 100%;
}

.toc-container .toc-title {
  font-weight: 700;
  font-size: 1.2em;
  margin-bottom: 0;
}

.toc-container li,
.toc-container ul,
.toc-container ul li {
  list-style: none !important;
}

.toc-container > ul {
  padding-left: 0;
}

.toc-container ul li span {
  display : flex;
}

.toc-container ul li span a {
  color: inherit;
  text-decoration: none;
}
.toc-container ul li span a:hover {
  color: inherit;
  text-decoration: none;
}

.toc-container ul li span span.dots {
  flex: 1;
  height: 0.65em;
  margin: 0 10px;
  border-bottom: 2px dotted black;
}

/*# sourceURL=webpack://./src/muya/lib/assets/styles/exportStyle.css */
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJzb3VyY2VzIjpbIndlYnBhY2s6Ly8uL3NyYy9tdXlhL2xpYi9hc3NldHMvc3R5bGVzL2V4cG9ydFN0eWxlLmNzcyJdLCJuYW1lcyI6W10sIm1hcHBpbmdzIjoiQUFBQTtFQUNFLHFCQUFxQjtBQUN2Qjs7QUFFQTtFQUNFLGdCQUFnQjtFQUNoQixXQUFXO0FBQ2I7O0FBRUE7RUFDRSxrQkFBa0I7QUFDcEI7O0FBRUE7RUFDRSxrQkFBa0I7RUFDbEIsb0JBQW9CO0VBQ3BCLFNBQVM7RUFDVCxVQUFVO0VBQ1YscUJBQXFCO0FBQ3ZCOztBQUVBOztFQUVFLHlCQUF5QjtBQUMzQjs7QUFFQTtFQUNFLFdBQVc7QUFDYjs7QUFFQTtFQUNFLGdCQUFnQjtFQUNoQixnQkFBZ0I7RUFDaEIsZ0JBQWdCO0FBQ2xCOztBQUVBOzs7RUFHRSwyQkFBMkI7QUFDN0I7O0FBRUE7RUFDRSxlQUFlO0FBQ2pCOztBQUVBO0VBQ0UsY0FBYztBQUNoQjs7QUFFQTtFQUNFLGNBQWM7RUFDZCxxQkFBcUI7QUFDdkI7QUFDQTtFQUNFLGNBQWM7RUFDZCxxQkFBcUI7QUFDdkI7O0FBRUE7RUFDRSxPQUFPO0VBQ1AsY0FBYztFQUNkLGNBQWM7RUFDZCwrQkFBK0I7QUFDakMiLCJzb3VyY2VSb290IjoiIn0= */</style>
  <style>.markdown-body{}pre.front-matter{display:none!important;}</style>
</head>
<body>
  <article class="markdown-body"><p><a href="./index.html">HOME</a>  </p>
<p><a href="https://arxiv.org/a/miyazaki_t_1.html">Preprints</a></p>
<h1 class="atx" id="journal">Journal</h1>
<ol reversed>
<li><p>Juan Wang, Zhijie Wang,  <u><strong>Tomo Miyazaki</strong></u> Yaohou Fan, Shinichiro Omachi<br>"TAMC: Textual Alignment and Masked Consistency for Open-Vocabulary 3D Scene Understanding"<br><em>Sensors</em>, vol. 24, no. 19, 6166, 2024  <a href="https://doi.org/10.3390/s24196166"><strong>[paper]</strong></a>  <strong>IF: 3.4</strong></p>
</li>
<li><p><a href="https://researchmap.jp/read0112297?lang=en">Hiroshi Hoshijima</a>, <u><strong>Tomo Miyazaki</strong></u>, Yuto Mitsui, Shinichiro Omachi, <a href="https://researchmap.jp/123412341234">Masanori Yamauchi</a>, <a href="https://researchmap.jp/read0138550">Kentaro Mizuta</a><br>"Machine learning-based identification of the risk factors for postoperative nausea and vomiting in adults"<br><em>PLOS ONE</em>, vol. 19, pp.1-15, 2024  <a href="https://doi.org/10.1371/journal.pone.0308755"><strong>[paper]</strong></a></p>
</li>
<li><p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Semantically-Guided Image Compression for Enhanced Perceptual Quality at Extremely Low Bitrates"<br><em>IEEE Access</em>, vol. 12, pp.100057-100072, 2024  <a href="https://doi.org/10.1109/ACCESS.2024.3430322"><strong>[paper]</strong></a>  <strong>IF: 3.4</strong></p>
</li>
<li><p><a href="https://hyongsong.work/">Yongsong Huang</a>, <u><strong>Tomo Miyazaki</strong></u>,  <a href="https://xliulab.mgh.harvard.edu/xiaofeng/">Xiaofeng Liu</a>, Kaiyuan Jiang, Zhengmi Tang, Shinichiro Omachi<br>"Learn From Orientation Prior for Radiograph Super-Resolution: Orientation Operator Transformer"<br><em>Computer Methods and Programs in Biomedicine</em>, pp.108000, 2024  <a href="https://doi.org/10.1016/j.cmpb.2023.108000"><strong>[paper]</strong></a>  <strong>IF: 6.1</strong></p>
</li>
<li><p>Takuru Ishikawa, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Japanese historical character recognition by focusing on character parts"<br><em>Pattern Recognition</em>, vol. 148, pp.110181, 2024  <a href="https://doi.org/10.1016/j.patcog.2023.110181"><strong>[paper]</strong></a>  <strong>IF: 8.518</strong></p>
</li>
<li><p>Zhengmi Tang, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"A Scene-Text Synthesis Engine Achieved Through Learning from Decomposed Real-World Data,"<br><em>IEEE Transactions on Image Processing</em>, vol. 32, pp.5837-5851, 2023 <a href="https://doi.org/10.1109/TIP.2023.3326685"><strong>[paper]</strong></a> <strong>IF: 10.6</strong></p>
</li>
<li><p>Akane Shoda, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"JPEG Image Enhancement with Pre-Processing of Color Reduction and Smoothing"<br><em>Sensors</em>, vol. 23, no.21, 8861, 2023 <a href="https://doi.org/10.3390/s23218861"><strong>[paper]</strong></a> <strong>IF: 3.9</strong></p>
</li>
<li><p>Kohei Kageyama, <u><strong>Tomo Miyazaki</strong></u>, <a href="https://researchmap.jp/read0055274">Yoshihiro Sugaya</a>, Shinichiro Omachi<br>"Collaborative Indoor Positioning by Localization Comparison at an Encounter Position"<br><em>Applied Science</em>, vol. 13, no. 12, 6962, 2023 <a href="https://doi.org/10.3390/app13126962"><strong>[paper]</strong></a>  <strong>IF: 2.838</strong></p>
</li>
<li><p>Shohei Uchigasaki, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Deep Image Compression Using Scene Text Quality Assessment"<br><em>Pattern Recognition</em>, vol. 142, pp.109696, 2023 <a href="https://arxiv.org/abs/2305.11373"><strong>[preprint]</strong></a> <a href="https://doi.org/10.1016/j.patcog.2023.109696"><strong>[paper]</strong></a>  <strong>IF: 8.518</strong></p>
</li>
<li><p><a href="https://researchmap.jp/myoshino">Masao Yoshino</a>, <a href="https://takashy57.wixsite.com/takashy57">Takashi Iida</a>, <a href="https://mzks.dev/">Keita Mizukoshi</a>, <u><strong>Tomo Miyazaki</strong></u>, <a href="https://researchmap.jp/kamadakei">Kei Kamada</a>, <a href="https://researchmap.jp/kjkim72">kyoung Jin Kim</a>, <a href="https://researchmap.jp/akria_yoshikawa">Akira Yoshikawa</a><br>"Comparative Pulse Shape Discrimination Study for Ca(Br, I)2 Scintillators Using Machine Learning and Conventional Methods"<br><em>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</em>, vol. 1045, pp. 167626, 2023 <a href="https://doi.org/10.1016/j.nima.2022.167626"><strong>[paper]</strong></a> <strong>IF: 1.335</strong></p>
</li>
<li><p>Taku Suzuki, Daisuke Sato, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Important Region Estimation Using Image Captioning"<br><em>IEEE Access</em>, vol. 10, pp.105546-105555, 2022 <a href="https://doi.org/10.1109/ACCESS.2022.3211260"><strong>[paper]</strong></a> <strong>IF: 3.476</strong></p>
</li>
<li><p>Kota Oodaira, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Importance Estimation for Scene Texts Using Visual Features"<br><em>Interdisciplinary Information Sciences</em>, Volume 28, Issue 1, pp.15-23, 2022 <a href="https://doi.org/10.4036/iis.2022.a.06"><strong>[paper]</strong></a></p>
</li>
<li><p>Shuya Sano, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Naohiro Sekiguchi, Shinichiro Omachi<br>"Mackerel Fat Content Estimation using RGB and Depth Images"<br><em>IEEE Access</em>, vol.9, pp.164060-164069, 2021 <a href="https://doi.org/10.1109/ACCESS.2021.3134260"><strong>[paper]</strong></a> <strong>IF: 3.367</strong></p>
</li>
<li><p>Zhengmi Tang, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Stroke-Based Scene Text Erasing Using Synthetic Data for Training"<br><em>IEEE Transactions on Image Processing</em>, vol.30, pp.9306-9320, 2021 <a href="https://doi.org/10.1109/TIP.2021.3125260"><strong>[paper]</strong></a> <strong>IF: 10.856</strong></p>
</li>
<li><p>Junpei Masuho, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, <a href="https://researchmap.jp/read0055275">Masako Omachi</a> and Shinichiro Omachi<br>"A Framework for Estimating Gaze Point Information for Location-Based Services"<br><em>IEEE Transactions on Vehicular Technology</em>, vol.70, no.9, pp.8468-8477, 2021 <a href="https://doi.org/10.1109/TVT.2021.3101932"><strong>[paper]</strong></a> <strong>IF: 5.978</strong></p>
</li>
<li><p>Sho Ishida, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Graph Neural Networks with Multiple Feature Extraction Paths for Chemical Property Estimation"<br><em>Molecules</em>, vol.26, no.11, 3125, 2021  <a href="https://doi.org/10.3390/molecules26113125"><strong>[paper]</strong></a> <strong>IF: 4.411</strong></p>
</li>
<li><p>Huy Manh Nguyen, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Multiple Visual-Semantic Embedding for Video Retrieval from Query
Sentence"<br><em>Applied Sciences</em>, vol.11, no.7, 3214, 2021 <a href="https://doi.org/10.3390/app11073214"><strong>[paper]</strong></a> <strong>IF: 2.679</strong></p>
</li>
<li><p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Text Detection Using Multi-Stage Region Proposal Network Sensitive to Text Scale"<br><em>Sensors</em>, vol.21, no.4, 1232, 2021 <a href="https://doi.org/10.3390/s21041232"><strong>[paper]</strong></a> <strong>IF: 3.576</strong></p>
</li>
<li><p>Antoine Chauvet, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Optical Flow-Based Fast Motion Parameters Estimation for Affine Motion Compensation"<br><em>Applied Sciences</em>, vol.10, no.2, 2020 <a href="https://doi.org/10.3390/app10020729"><strong>[paper]</strong></a> <strong>IF: 2.679</strong></p>
</li>
<li><p>Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br> "Object-Based Video Coding by Visual Saliency and Temporal Correlation"<br> <em>IEEE Transactions on Emerging Topics in Computing (TETC)</em>, Volume 8, Issue 1, pp.168-178, 2020 <a href="https://doi.org/10.1109/TETC.2017.2695640"><strong>[paper]</strong></a> <strong>IF: 7.691</strong></p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro Omachi, <a href="http://www.m.cs.osakafu-u.ac.jp/~masa/index-e_old.shtml">Masakazu Iwamura</a>, <a href="http://human.ait.kyushu-u.ac.jp/~uchida/index-e.html">Seiichi Uchida</a>, <a href="http://www.m.cs.osakafu-u.ac.jp/~kise/index_e.html">Koichi Kise</a><br> "Automatic Generation of Typographic Font from a Small Font Subset"<br> <em>IEEE Computer Graphics and Applications</em>, vol.40, issue 1, pp.99-111, 2020 <a href="https://doi.org/10.1109/MCG.2019.2931431"><strong>[paper]</strong></a> <strong>IF: 2.088</strong></p>
</li>
<li><p>Antoine Chauvet, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br> "Fast Image Quality Enhancement for HEVC by Post Filtering using Shallow Neural Networks"<br> <em>IIEEJ Transactions on Image Electronics and Visual Computing</em>, vol.7, no.1, p.2-12, 2019 <a href="https://doi.org/10.11371/tievciieej.7.1_2"><strong>[paper]</strong></a></p>
</li>
<li><p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya,  Shinichiro Omachi<br> "Automatic Mackerel Sorting Machine using Global and Local Features"<br> <em>IEEE Access</em>, vol. 7, pp. 63767-63777, 2019 <a href="https://doi.org/10.1109/ACCESS.2019.2917554"><strong>[paper]</strong></a> <a href="https://github.com/yoyoyo-yo/glcc-frcn.pytorch"><strong>[code]</strong></a> <strong>IF: 3.367</strong></p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br> "Structural Data Recognition with Graph Model Boosting"<br> <em>IEEE Access</em>, vol.6, pp.63606-63618, 2018 <a href="https://doi.org/10.1109/ACCESS.2018.2876860"><strong>[paper]</strong></a> <strong>IF: 3.367</strong></p>
</li>
<li><p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br> "Activity Recognition Using Gazed Text and Viewpoint Information for User Support Systems"<br> <em>Journal of Sensor and Actuator Networks</em>, Volume 7, Issue 3, p.31, 2018 <a href="https://doi.org/10.3390/jsan7030031"><strong>[paper]</strong></a></p>
</li>
<li><p>Airi Kitasato, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br> "Automatic Discrimination between Scomber japonicus and Scomber australasicus by Geometric and Texture Features"<br> <em>Fishes</em>, Volume 3, Issue 3, p.26, 2018 <a href="https://doi.org/10.3390/fishes3030026"><strong>[paper]</strong></a> <strong>IF: 2.385</strong></p>
</li>
<li><p>Yosuke Nozue, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br> "Efficient Coding for Video Including Text Using Image Generation"<br> <em>Journal of Information Processing</em>, vol.24, no.2, pp.330-338, 2016 <a href="https://doi.org/10.2197/ipsjjip.24.330"><strong>[paper]</strong></a>  </p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br> "Representative Graph Generation for Graph-Based Character Recognition"<br> <em>The Journal of the Institute of Image Electronics Engineers of Japan</em>, vol.40, no.3, pp.439-447, 2011 <a href="https://doi.org/10.11371/iieej.40.439"><strong>[paper]</strong></a></p>
</li>
</ol>
<p>Japanese:</p>
<ol reversed>
<li><p>菅谷 至寛, 坂井 清士郎, <u><strong>宮崎 智</strong></u>, 大町 真一郎<br>"環境中文字認識を利用した情報提供アプリケーションのための
ウェアラブルシステムの開発"<br>画像電子学誌 第48巻 第2号, pp.248-257, 2019 <a href="https://doi.org/10.11371/iieej.48.248"><strong>[paper]</strong></a></p>
</li>
<li><p>川村 思織, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"補助線を用いた情景画像からの高精度文字領域抽出"<br>画像電子学会誌, vol.45, no.1, pp.62-70, 2016 <a href="https://doi.org/10.11371/iieej.45.62"><strong>[paper]</strong></a>  </p>
</li>
<li><p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>"重み付き最小共通スーパーグラフを用いたシルエット画像認識"<br>画像電子学会誌, vol.38, no.5, pp.640-647, 2009 <a href="https://doi.org/10.11371/iieej.38.640"><strong>[paper]</strong></a></p>
</li>
</ol>
<p>Misc:</p>
<ol reversed>
<li><p><a href="http://researchmap.jp/kitamoto/">北本 朝展</a>, <a href="https://researchmap.jp/tkasasagi">カラーヌワット タリン</a>, <u><strong>宮崎 智</strong></u>, <a href="https://researchmap.jp/read0042033">山本　和明</a><br>"文字データの分析――機械学習によるくずし字認識の可能性とそのインパクト――"<br>電子情報通信学会会誌, Vol.102 No.6pp.563-568, 2019 <a href="https://www.journal.ieice.org/summary.php?id=k102_6_563&amp;year=2019&amp;lang=J"><strong>[paper]</strong></a>   </p>
</li>
<li><p><u><strong>宮崎 智</strong></u>, 川村 思織, 菅谷 至寛, 大町 真一郎<br>"ユーザ入力の補助線による情景画像からの高精度文字抽出"<br>画像ラボ, 2017年10月号, pp.37-42</p>
</li>
</ol>
<h1 class="atx" id="international-conference-refereed">International Conference (Refereed)</h1>
<ol reversed>
<li><p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Controlling Rate, Distortion, and Realism: Towards a Single Comprehensive Neural Image Compression Model"<br>Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</p>
</li>
<li><p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>"Self Texture Transfer Networks for Low Bitrate Image Compression"<br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp.1901-1905, 2021
<a href="https://doi.org/10.1109/CVPRW53098.2021.00214"><strong>[paper]</strong></a></p>
</li>
<li><p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>"Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks"<br>Proceedings of the 25th International Conference on Pattern Recognition (ICPR 2020), pp.8235-8242, Jaunary 2021.
<a href="https://arxiv.org/abs/2008.10314"><strong>[arxiv]</strong></a>
<a href="https://github.com/iwa-shi/fidelity_controllable_compression"><strong>[code]</strong></a>
<a href="https://doi.org/10.1109/ICPR48806.2021.9412185"><strong>[paper]</strong></a></p>
</li>
<li><p>Xi Huang, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>"Super Resolution for Multi Frames with 3D Feature Extraction and RNN Prediction"<br>Proceedings of the 2019 International Symposium on Signal Processing Systems (SSPS 2019), pp.82-86, September 2019. <a href="https://doi.org/10.1145/3364908.3364909"><strong>[paper]</strong></a></p>
</li>
<li><p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya,  Shinichiro Omachi<br>"Mackerel Classification using Global and Local Features"<br>The 2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA), p.1209-1212, 2018 <a href="https://doi.org/10.1109/ETFA.2018.8502584"><strong>[paper]</strong></a></p>
</li>
<li><p>Chisato Sugawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Text Retrieval for Japanese Historical Documents by Image Generation"<br>The 4th International Workshop on Historical Document Imaging and Processing (HIP), p.19-24, 2017 <a href="https://doi.org/10.1145/3151509.3151512"><strong>[paper]</strong></a></p>
</li>
<li><p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Text Detection by Faster R-CNN with Multiple Region Proposal Network"<br>International Workshop on Camera Based Document Analysis and Recognition (CBDAR), p.15-20, 2017 <a href="https://doi.org/10.1109/ICDAR.2017.343"><strong>[paper]</strong></a></p>
</li>
<li><p>Yoshihiro Sugaya, Kento Takeda, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"A Preliminary Study on Location Estimation without Preparation using Ceiling Signboard"<br>International Conference on Indoor Positioning and Indoor Navigation (IPIN), 179_WIP, 2017 </p>
</li>
<li><p>Ofusa Kenichiro, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Glyph-Based Data Augmentation for Accurate Kanji Character Recognition"<br>International Conference on Document Analysis and Recognition (ICDAR), p.597-602, 2017 <a href="https://doi.org/10.1109/ICDAR.2017.103"><strong>[paper]</strong></a></p>
</li>
<li><p>Tomoya Honto, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Analysis of Floor Map Image in Information Board for Indoor Navigation"<br>International Conference on Indoor Positioning and Indoor Navigation (IPIN), 144_RP, 2017 <a href="https://doi.org/10.1109/IPIN.2017.8115896"><strong>[paper]</strong></a></p>
</li>
<li><p>Kiyoshiro Sakai, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Development of Wearable System for Translation of Japanese Texts in the Environment"<br>Proceedings of International Workshop on Frontiers of Computer Vison (FCV), OS3-2, 2017</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Graph Model Boosting for Structural Data Recognition"<br>Proceedings of International conference of Pattern Recognition (ICPR), p.1708-1713, 2016 <a href="http://dx.doi.org/10.1109/ICPR.2016.7899882"><strong>[paper]</strong></a>  </p>
</li>
<li><p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Improvement of Map Matching for Indoor Navigation Exploiting Photo of Information Board"<br>Proceedings of International Conference on Indoor Positioning and Indoor Navigation (IPIN), 219-WIP, 2016</p>
</li>
<li><p>Antoine Chauvet, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya Shinichiro Omachi<br>"Adaptive Post Filter for Reducing Block Artifacts in High Efficiency Video Coding"<br>International Conference on Multimedia Systems and Signal Processing, 2016 <a href="http://dx.doi.org/10.1109/ICMSSP.2016.014"><strong>[paper]</strong></a></p>
</li>
<li><p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Indoor Localization by Map Matching Using One Image of Information Board"<br>Proceedings of The Eighth International Conferences on Pervasive Patterns and Applications (PATTERNS), pp.22-26, 2016<br><a href="http://www.iaria.org/conferences2016/AwardsPATTERNS16.html"><strong>[<font color="red">Best Paper Award</font>]</strong></a> <a href="http://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2016_1_40_70039"><strong>[paper]</strong></a></p>
</li>
<li><p>Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Object-based Video Coding for Arbitrary Shape by Visual Saliency and Temporal Correlation"<br>Proceedings of The Eighth International Conference on Creative Content Technologies (CONTENT), pp.13-16, 2016<br><a href="http://www.iaria.org/conferences2016/AwardsCONTENT16.html"><strong>[<font color="red">Best Paper Award</font>]</strong></a> <a href="http://www.thinkmind.org/index.php?view=article&amp;articleid=content_2016_1_30_60053"><strong>[paper]</strong></a></p>
</li>
<li><p>Airi Kitasato, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Discrimination of Scomber Japonicus and Scomber Australasicus by Dorsal Fin Length and Fork Length"<br>Proceedings of The 22nd Korea-Japan joint Workshop on Frontiers of Computer Vision (FCV), pp.338-341, 2016</p>
</li>
<li><p>Shuhei Toba, Hirotaka Kudo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Ultra-low Resolution Character Recognition System with Pruning Mutual Subspace Method"<br>Proceedings of International Conference on Consumer Electronics - Taiwan (ICCE-TW), pp.284-285, 2015 <a href="http://dx.doi.org/10.1109/ICCE-TW.2015.7216900"><strong>[paper]</strong></a></p>
</li>
<li><p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Estimation of Gazing Points in Environment Using Eye Tracker and Omnidirectional Camera"<br>Proceedings of 2015 International Conference on Consumer Electronics - Taiwan (ICCE-TW), pp.47-48, 2015 <a href="http://dx.doi.org/10.1109/ICCE-TW.2015.7217003"><strong>[paper]</strong></a></p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Finding Stroke Parts for Rough Text Detection in Scene Images with Random Forest"<br>Proceedings of Joint Conference of IWAIT and IFMIA, 2015</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Fast Method for Extracting Representative Graph from Decorative Character Images"<br>Proceedings of IEEE International Conference on Network Infrastructures and Digital Content (IEEE IC-NIDC), pp.219-223, 2010 <a href="http://dx.doi.org/10.1109/ICNIDC.2010.5657776"><strong>[paper]</strong></a></p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Extraction of Representative Structure of Decorative Character Images"<br>Proceedings of the Chinese Conference on Pattern Recognition (CCPR) and the First CJK Joint Workshop on Pattern Recognition (CJKPR), vol.2, pp.944-948, 2009 <a href="http://dx.doi.org/10.1109/CCPR.2009.5343952"><strong>[paper]</strong></a></p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Extraction of Structure of Silhouette Images by Weighted Minimum Common Supergraph"<br>Proceedings of the Second Korea-Japan Joint Workshop on Pattern Recognition (KJPR), pp.57-61, 2007</p>
</li>
</ol>
<h1 class="atx" id="international-conference-non-refereed">International Conference (Non-refereed)</h1>
<ol reversed>
<li><p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>"Two-Stage Training for High-Fidelity Image Compression with Generative Adversarial Networks"<br>International Workshops on Emerging ICT, 2020 (Oral)</p>
</li>
<li><p>Kyoko Maeda,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Investigation of Convolutional Neural Network Structure for Low Resolution Character Recognition"<br>International Workshop on Emerging ICT, 2016</p>
</li>
<li><p>Kota Oodaira,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Detection of a Key String from Scene Images Using Saliency"<br>International Workshop on Emerging ICT, 2016</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Rough Detection of Text in Scene Images by Finding Stroke Parts"<br>Proceedings of the International Workshop on Electronics and Communications, Oral Session 4, 2014</p>
</li>
<li><p>Jian Wang, Hiroya Saito, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Automatic Synthesis of Character Pattern Using Patch Transform"<br>Proceedings of the International Workshop on Electronics and Communications, Oral Session 1, 2014</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Extracting Representative Graph of Decorative Character Images by Random Method"<br>Proceedings of the Third Student Organizing International Mini-Conference on Information Electronics Systems, pp.67–68, 2010</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>“Iterative Extraction of Representative Graph Using
Common Features from Decorative Character Images,”<br>Proceedings of the Second Student Organizing International Mini-Conference on Information Electronics Systems,
pp.85–86, 2009</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Structure Extraction from Silhouette Images by Weighted Minimum Common Supergraph"<br>Proceedings of the First Student Organizing International Mini-Conference on Information Electronics Systems, pp.75–76, 2008</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Silhouette Image Recognition"<br>Proceedings of the Third Korea-Japan Joint Workshop on Pattern Recognition, pp.13–14, 2008.</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Extraction of Structure of Shapes Using Weighted Minimum Common Supergraph"<br>Proceedings of the China-Korea-Japan Graduates Workshop on Electronic Information, pp.49–50, 2008</p>
</li>
</ol>
<h1 class="atx" id="domestic-non-refereed">Domestic (Non-refereed)</h1>
<ol reversed>
<li><p>岩井翔真, 宮崎智, 大町真一郎<br>"画像圧縮におけるVQGAN活用のための双条件付き学習"<br>第27回 画像の認識・理解シンポジウム, OS-2C-03, 2024 <a href="https://miru-committee.github.io/miru2024/author/award/"><strong>[<font color="red">MIRU学生奨励賞</font>]</strong></a></p>
</li>
<li><p>高野寛己, 高久悠杜, 太田晋一 (宮城県産業技術総合センター), 宮崎智, 大町真一郎<br>"生成画像を用いた工業画像マルチクラス分類と異常検知アルゴリズム性能との比較検証"<br>第27回 画像の認識・理解シンポジウム, IS-2-034, 2024 <a href="https://miru-committee.github.io/miru2024/author/award/"><strong>[<font color="red">MIRUインタラクティブ発表賞</font>]</strong></a></p>
</li>
<li><p>Taiga Goncalves, Tomo Miyazaki, Shinichiro Omachi<br>"Domain Generalizable Multi-Targeted Adversarial Attack using Dynamic Loss Weighting"<br>第27回 画像の認識・理解シンポジウム, IS-1-075, 2024 <a href="https://miru-committee.github.io/miru2024/author/award/"><strong>[<font color="red">MIRUインタラクティブ発表賞</font>]</strong></a></p>
</li>
<li><p>石川健志, 宮崎智, 大町真一郎<br>"Deepfake text検出：生成AIによるテキスト編集の検出手法とデータセットの開発"<br>第27回 画像の認識・理解シンポジウム, IS-1-031, 2024</p>
</li>
<li><p>安孫子理人, 宮崎智, 大町真一郎 (東北大), 藤井康寿 (Google DeepMind)<br>"品質評価モデルと拡散過程を用いた情景画像中テキストの可読性評価"<br>第27回 画像の認識・理解シンポジウム, IS-1-041, 2024</p>
</li>
<li><p>高橋翔, 宮崎智, 大町真一郎<br>"インスタンスセグメンテーションを用いた複雑性除去による異常検知手法"<br>第27回 画像の認識・理解シンポジウム, IS-1-089, 2024</p>
</li>
<li><p>尾崎瞬, 宮崎智, 大町真一郎, 藤井康寿 (Google DeepMind)
"フレームのキャプショニングを用いた動画要約学習"<br>第27回 画像の認識・理解シンポジウム, IS-1-107, 2024</p>
</li>
<li><p>Juan Wang, ZhiJie Wang (RIKEN AIP), Tomo Miyazaki, Shinichiro Omachi (Tohoku Univ.)<br>"Improved Open-Vocabulary 3D Scene Understanding via Masked Feature Alignment"<br>第27回 画像の認識・理解シンポジウム, IS-1-020, 2024</p>
</li>
<li><p>東桔也, 宮崎智, 大町真一郎<br>"連合学習の通信量を削減するためのEnergyスコアを利用した知識蒸留手法の検討"<br>電子情報通信学会総合大会学生ポスターセッション, 2024 <strong>[<font color="red">優秀ポスター賞</font>]</strong></p>
</li>
<li><p>岩井翔真, 宮崎智, 大町真一郎<br>"学習済みVQGANを活用した深層画像符号化手法の検討"<br>画像符号化シンポジウム/映像メディア処理シンポジウム（PCSJ/IMPS), 2023 <strong>[<a href="https://www.pcsj-imps.org/archive/2023awards.html"><font color="red">ベストポスター賞</font></a>]</strong></p>
</li>
<li><p>高野寛己(宮城県産業技術総合センター), 太田晋一 (宮城県産業技術総合センター), 宮崎智, 大町真一郎<br>"不整列データセットを用いた異常検知アルゴリズムの性能検証"<br>第26回 画像の認識・理解シンポジウム, IS1-42, 2023</p>
</li>
<li><p>阿部楓也, 岩井翔真, 宮崎智, 大町真一郎<br>"生成画像を利用した少数データくずし字認識に関する検討"<br>第26回 画像の認識・理解シンポジウム, IS1-66, 2023</p>
</li>
<li><p>木下純哉, 宮崎智, 大町真一郎<br>"パーツプロトタイプを用いたくずし字認識に関する検討"<br>第26回 画像の認識・理解シンポジウム, IS1-91, 2023</p>
</li>
<li><p>岩井翔真, 宮崎智, 大町真一郎<br>"GANを使ったマルチレート画像符号化モデルのための学習戦略の検討"<br>第26回 画像の認識・理解シンポジウム, IS2-63, 2023</p>
</li>
<li><p>三ツ井悠翔，宮崎智，大町真一郎<br>"Masked Image Modeling を利用した情景画像中のテキスト認識"<br>言語処理学会第29 回年次大会(NLP2023), pp.2340-2343, 2023</p>
</li>
<li><p>恩田 浩志, 宮崎 智, 大町 真一郎<br>"説明文からのテキスト領域を含む画像の生成に関する検討"<br>第25回 画像の認識・理解シンポジウム, IS1-58, 2022</p>
</li>
<li><p>谷本 衡哉, 大町 真一郎, 宮崎 智<br>"U-Netを用いた電子部品画像からの配線検出"<br>第25回 画像の認識・理解シンポジウム, IS1-71, 2022</p>
</li>
<li><p>石川 太繰, 宮崎 智, 大町 真一郎<br>"古典籍中の少数サンプル文字種認識のための文字部位認識"<br>第25回 画像の認識・理解シンポジウム, IS2-68, 2022</p>
</li>
<li><p>宇川 慧, 宮崎 智, 大町 真一郎
"物体のテクスチャに注目したGANによる熱赤外線画像着色手法の検討"<br>第25回 画像の認識・理解シンポジウム, IS3-52, 2022</p>
</li>
<li><p>阿部 楓也, 岩井 翔真, 宮崎 智, 大町 真一郎<br>"少数くずし字データ補強のための画像生成に関する検討"<br>第25回 画像の認識・理解シンポジウム, IS3-53, 2022</p>
</li>
<li><p>岩井 翔真, 宮崎 智, 大町 真一郎<br>GANを使った画像符号化における識別器に関する検討<br>第25回 画像の認識・理解シンポジウム, IS3-56, 2022</p>
</li>
<li><p>梶原 颯希, 宮崎 智, 大町 真一郎<br>"編集距離に基づいた距離学習による情景画像文字認識結果の修正に関する検討"<br>第25回 画像の認識・理解シンポジウム, IS3-80, 2022</p>
</li>
<li><p>内ヶ崎翔平, 菅谷至寛, 宮崎智, 大町真一郎<br>"文字の可読性を考慮した画像の高効率符号化手法の実験的検証"<br>第24回 画像の認識・理解シンポジウム, I21-34, 2021</p>
</li>
<li><p>石川太繰, 宮崎智, 菅谷至寛, 大町真一郎
"文字部位に着目した古典籍くずし字認識に関する基礎検討"<br>第24回 画像の認識・理解シンポジウム, I31-35, 2021</p>
</li>
<li><p>岩井翔真, <u><strong>宮崎智</strong></u>, 菅谷至寛, 大町真一郎<br>"参照型超解像による特徴転移を利用した高品質な画像符号化"<br>第24回 画像の認識・理解シンポジウム, I32-04, 2021</p>
</li>
<li><p>Kohei Kageyama, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Improvement of the Usability for Indoor Navigation Exploiting Images of Information Boards"<br>2021年度電気関係学会東北支部連合大会講演論文集, 2021</p>
</li>
<li><p>本田大智, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>"歪みのあるフロアマップ上での慣性センサを用いた位置推定法の検討"<br>2020年度電気関係学会東北支部連合大会講演論文集, S04, 2020</p>
</li>
<li><p>西村 遼平,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"ニューラルネットワークを用いた古典籍画像からの文字検出"<br>電子情報通信学会総合大会学生ポスターセッション, 2F17, 2019</p>
</li>
<li><p>石田 聖,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"グラフ構造に着目した分子特性の認識"<br>電子情報通信学会総合大会学生ポスターセッション, 2F19, 2019</p>
</li>
<li><p>中屋 悠資, 鈴木 海斗,&nbsp;<u><strong>宮崎 智</strong></u>, 大町 真一郎<br>"部首に注目したDeep Leaningによるくずし字の認識を用いた日本古典籍の解析"<br>電子情報通信学会総合大会学生ポスターセッション, XXX, 2019</p>
</li>
<li><p>Nguyen Manh Huy, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>"Joint Video and Language Understanding with Visual-Semantic Embedding"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1H10, 2019<br><a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/awards2019.pdf"><strong>[<font color="red">The Best Paper Prize</font>]</strong></a></p>
</li>
<li><p>増保純平, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 大町真一郎<br>"ウェアラブルセンサと固定カメラを用いたユーザの注目位置推定"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1F15, 2019</p>
</li>
<li><p>小泉翔太, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>"略地図画像からの道路領域抽出手法の検討"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1F12, 2019</p>
</li>
<li><p>竹村貴文, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>"屋内撮影画像中の案内板までの実距離推定手法の検討"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1F11, 2019</p>
</li>
<li><p>桑野拓朗, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>"画像生成を取り入れた適応的符号化に関する検討"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1F08, 2019</p>
</li>
<li><p>長崎 大, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 大町真一郎<br>"超解像ネットワークを利用した画像符号化手法の検討"<br>2019年度電気関係学会東北支部連合大会講演論文集, 1F07, 2019<br><a href="https://www.ieice.org/tohoku/award/images/2019awards.pdf"><strong>[<font color="red">学生優秀論文賞</font>]</strong></a></p>
</li>
<li><p>黄 希, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"Multi-Frame Super Resolution Using 3D Convolution and RNN Prediction"<br>平成30年度電気関係学会東北支部連合大会講演論文集, 2F12, 2018<br><a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/awards2018.pdf"><strong>[<font color="red">The Encouragement Prize</font>]</strong></a></p>
</li>
<li><p>桑野 拓朗, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"超解像を用いた動画像符号化に関する検討"<br>平成30年度電気関係学会東北支部連合大会講演論文集, 1D14, 2018</p>
</li>
<li><p>増保 純平, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"監視カメラを活用したユーザの実世界位置の推定"<br>平成30年度電気関係学会東北支部連合大会講演論文集, 1I05, 2018</p>
</li>
<li><p>竹村 貴文, 菅谷 至寛, <u><strong>宮崎 智</strong></u>, 大町 真一郎<br>"天吊り案内板を用いた屋内ナビゲーション手法の検討"<br>平成30年度電気関係学会東北支部連合大会講演論文集, 2A07, 2018</p>
</li>
<li><p>佐藤 大亮, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"Determining Important Objects in Scene Image Using Neural Networks"<br>平成29年度電気関係学会東北支部連合大会講演論文集, 2B12, 2017<br><a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2017.html"><strong>[<font color="red">The Encouragement Prize</font>]</strong></a></p>
</li>
<li><p>八重樫 日菜子,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"スパースコーディングを用いた動画像符号化に関する検討"<br>電子情報通信学会総合大会学生ポスターセッション, ISS-SP-200, 2017</p>
</li>
<li><p>大平 康太,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"重要度を考慮した情景画像中における文字情報抽出"<br>電子情報通信学会総合大会学生ポスターセッション, ISS-SP-199, 2017</p>
</li>
<li><p>坂井 清士郎,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"文字認識を利用した環境中の日本語英字翻訳を行うウェアラブルシステムの開発"<br>平成28年度電気関係学会東北支部連合大会講演論文集, 2G04, 2016</p>
</li>
<li><p>Kyoko Maeda,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Low Resolution Character Recognition Using Convolutional Neural Networks"<br>平成28年度電気関係学会東北支部連合大会講演論文集, 1A04, 2016</p>
</li>
<li><p>Kenta Takeda,&nbsp;Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Signboard Extraction and Recognition in Subway Station Premises"<br>平成28年度電気関係学会東北支部連合大会講演論文集, 1A03, 2016</p>
</li>
<li><p>井上 慶祐,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"スパースコーディングを用いたテキストを含む画像符号化に関する検討"<br>電子情報通信学会技術研究報告, IE2016-36, vol.116, no.119, pp.5-10, 2016</p>
</li>
<li><p>景山 竣, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"パーツの生成による少数サンプルからのフォント生成"<br>画像の認識・理解シンポジウム (MIRU), PS3-08, 2016</p>
</li>
<li><p>大平 康太, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"顕著性を利用した情景画像からの重要な文字列の検出"<br>画像の認識・理解シンポジウム (MIRU), PS3-42, 2016</p>
</li>
<li><p>Toshiaki Sakai, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Graph Learning with Quadratic Programming in Consideration of Class Diversity"<br>画像の認識・理解シンポジウム (MIRU), PS2-42, 2016</p>
</li>
<li><p>酒井 利晃, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"非線形最小化によるグラフのモデルの構築と画像認識"<br>電子情報通信学会2016年総合大会講演論文集, D-12-96, 2016</p>
</li>
<li><p>Shuto Shinbo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Accuracy Improvement of Character Recognition Using Generated Samples by Morphing"<br>平成27年度電気関係学会東北支部連合大会講演論文集, 2A12, 2015<br><a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2015.html"><strong>[<font color="red">The Best Paper Prize</font>]</strong></a></p>
</li>
<li><p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Gaze Detection in Omnidirectional Scene by Iterative Image Matching"<br>平成27年度電気関係学会東北支部連合大会講演論文集, 2A11, 2015</p>
</li>
<li><p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Survey of Guide Plates and Fundamental Study of Map Image Analysis for Indoor Navigation"<br>平成27年度電気関係学会東北支部連合大会講演論文集, 1E05, 2015</p>
</li>
<li><p>小笠原 和也, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"顕著性マップとGrabCutによる注目物体抽出を用いた動画像符号化"<br>平成27年度電気関係学会東北支部連合大会講演論文集, 2D10, 2015</p>
</li>
<li><p>千葉 駿, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"アイトラッカと全方位カメラを用いた環境中の視点位置推定"<br>電子情報通信学会技術研究報告, PRMU2014-134, vol.114, no.454, pp.101-102, 2015</p>
</li>
<li><p>大島 康嗣, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"ストローク幅特徴を用いた情景画像中の文字検出"<br>電子情報通信学会技術研究報告, PRMU2014-133, vol.114, no.454, pp.99-100, 2015</p>
</li>
<li><p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"画像抽象化による分割圧縮効果改善の検討"<br>電子情報通信学会技術研究報告, PRMU2014-132, vol.114, no.454, pp.97-98, 2015</p>
</li>
<li><p>鳥羽 修平, 工藤 裕貴, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"動画像を用いた超低解像度文字認識"<br>電子情報通信学会技術研究報告, PRMU2014-131, vol.114, no.454, pp.95-96, 2015</p>
</li>
<li><p>吉田 大樹, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"電子基板の欠陥検査のための文字認識"<br>平成27年度電気関係学会東北支部連合大会講演論文集, 1E06, 2015</p>
</li>
<li><p>川村 思織, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"補助線を利用した文字領域抽出"<br>電子情報通信学会技術研究報告, PRMU2013-152, vol.113, no.431, pp.119-120, 2014</p>
</li>
<li><p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"オブジェクトベースの符号化のための画像抽象化を用いた分割圧縮"<br>画像符号化シンポジウム予稿集, P-4-11, 2014</p>
</li>
<li><p>野末 洋佑, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"文字パラメータを利用した動画像中の文字の高効率符号化"<br>画像符号化シンポジウム予稿集, P-2-11, 2014</p>
</li>
<li><p>Shuhei Toba, Hirotaka Kudo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Ultra-Low Resolution Character Recognition with Increased Training Data and Image Enhancement"<br>平成26年度電気関係学会東北支部連合大会講演論文集, 2A18, 2014</p>
</li>
<li><p>Tatsunori Tsuchiya, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Automatic Generation of Kanji Fonts from Sample Designs"<br>平成26年度電気関係学会東北支部連合大会講演論文集, 2A17, 2014</p>
</li>
<li><p>Yosuke Nozue, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"A Video Coding Method for Scene Text"<br>平成26年度電気関係学会東北支部連合大会講演論文集, 2A19, 2014</p>
</li>
<li><p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"抽象化を用いた画像圧縮のための領域分割"<br>平成26年度電気関係学会東北支部連合大会講演論文集, 2H14, 2014</p>
</li>
<li><p>大島 康嗣,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"全方位情景画像のための2段階文字列検出"<br>画像の認識・理解シンポジウム (MIRU), SS1-42, 2014</p>
</li>
<li><p>鳥羽 修平, 工藤 裕貴,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"ぼけ除去及び複数フレームを利用した超低解像度文字認識"<br>画像の認識・理解シンポジウム (MIRU), SS2-50, 2014</p>
</li>
<li><p>土屋 達徳,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"サンプルデザインからの漢字フォントの自動生成"<br>画像の認識・理解シンポジウム (MIRU), SS3-17, 2014</p>
</li>
<li><p>川村 思織,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>"補助線を利用した文字領域抽出"<br>画像の認識・理解シンポジウム (MIRU), SS3-45, 2014</p>
</li>
<li><p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Structure Extraction from Multiple Decorative Character Images"<br>平成21年度電気関係学会東北支部連合大会講演論文集, 2A-11, 2009<br><a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2009.html"><font color="red">The Best Paper Prize</font></a></p>
</li>
<li><p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>"共通拡大グラフによる飾り文字画像群からの文字構造抽出"<br>画像の認識・理解シンポジウム論文集, IS1-16, pp.506-512, 2009</p>
</li>
<li><p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>"重み付き最小共通スーパーグラフによるシルエット画像の構造抽出"<br>画像の認識・理解シンポジウム論文集, IS5-3, pp.1408-1413, 2008</p>
</li>
</ol>
<h1 class="atx" id="exhibition">Exhibition</h1>
<ol reversed>
<li>全方位文字認識技術（全方位に存在する文字を高精度かつリアルタイムに認識するデモシス
テム）の出展, 最先端IT・エレクトロニクス総合展（CEATEC JAPAN）, 2014 年10 月
7–8 日</li>
<li>コンピュータと人が融和する文字・文書メディアの利用技術（超低解像度文字を高精度に認
識するデモシステム）の出展, 東北大学イノベーションフェア, 2014 年1 月28 日</li>
<li>人間調和型の文字検出および認識手法（人間が見た文字を検出し認識するデモシステム）の
出展, 東北大学電気・情報東京フォーラム, 2013 年11 月25 日</li>
</ol>
<h1 class="atx" id="awards">Awards</h1>
<ol reversed>
<li><p>石田實記念財団研究奨励賞, 2021 <a href="./imgs/ishida_2021.pdf"><strong>[賞状]</strong></a></p>
</li>
<li><p>The Eighth International Conferences on Pervasive Patterns and Applications, Best Paper Award (March 22, 2016)<br>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>"Indoor Localization by Map Matching Using One Image of Information Board"<br>Proceedings of The Eighth International Conferences on Pervasive Patterns and Applications (PATTERNS), pp.22-26, 2016 <a href="http://www.iaria.org/conferences2016/AwardsPATTERNS16.html"><strong>[URL]</strong></a>   </p>
</li>
<li><p>The Eighth International Conferences on Creative Content Technologies, Best Paper Award (March 22, 2016)<br>Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>"Object-based Video Coding for Arbitrary Shape by Visual Saliency and Temporal Correlation"<br>Proceedings of The Eighth International Conference on Creative Content Technologies (CONTENT), pp.13-16, 2016 <a href="http://www.iaria.org/conferences2016/AwardsCONTENT16.html"><strong>[URL]</strong></a>  </p>
</li>
<li><p>IEEE Sendai Section, The Best Paper Prize (December 7, 2009)<br><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>"Structure Extraction from Multiple Decorative Character Images"<br>Tohoku-Section Joint Convention Record of Institutes of Electrical and Information Engineers, 2A-11, 2009 <a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2009.html"><strong>[URL]</strong></a></p>
</li>
</ol>
<h1 class="atx" id="patent">Patent</h1>
<ol reversed>
<li><p>特願2020-152161，特開2022-046226, 特許第7535731号<br>魚脂肪量推定システム、魚脂肪量推定装置、及び、魚脂肪量推定方法<br>大町真一郎, 佐野修弥, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 関口尚大, 山下龍麿, 横山桂一郎, 本田光正<br>2020年9月10日出願，2022年3月23日公開</p>
</li>
<li><p>特願2018-018601，特開2019-135624, 特許第6890789号<br>魚選別装置、魚選別方法、魚種類推定装置、及び、魚種類推定方法<br>大町真一郎, 長岡禎人, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 山下龍麿, 菅原道晴, 小野寺政行<br>2018年2月5日出願，2019年8月15日公開</p>
</li>
<li><p>特願2016-056139，特開2017-173001，特許第6653507号<br>情報端末、位置推定方法、および位置推定プログラム<br>菅谷 至寛, 外崎 健人, 大町 真一郎,&nbsp;<u><strong>宮崎 智</strong></u><br>2016年3月18日出願, 2017年9月28日公開</p>
</li>
</ol>
</article>
</body>
</html>