<!DOCTYPE html>
<html data-markdown-preview-plus-context="html-export">
  <head>
    <meta charset="utf-8" />
    <title>publications</title>
    <style>.emoji {
  max-width: 1em !important;
}
del {
  text-decoration: none;
  position: relative;
}
del::after {
  border-bottom: 1px solid black;
  content: '';
  left: 0;
  position: absolute;
  right: 0;
  top: 50%;
}
ul.contains-task-list li.task-list-item {
  position: relative;
  list-style-type: none;
}
ul.contains-task-list li.task-list-item input.task-list-item-checkbox {
  position: absolute;
  transform: translateX(-100%);
  width: 30px;
}
span.critic.comment {
  position: relative;
}
span.critic.comment::before {
  content: '\1f4ac';
  position: initial;
}
span.critic.comment > span {
  display: none;
}
span.critic.comment:hover > span {
  display: initial;
  position: absolute;
  top: 100%;
  left: 0;
  border: 1px solid;
  border-radius: 5px;
  max-height: 4em;
  overflow: auto;
}
span.critic.comment:focus > span {
  display: initial;
  text-decoration: underline;
  position: initial;
  top: auto;
  left: auto;
  border: initial;
  border-radius: initial;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
  background-color: transparent;
}

body {
  overflow: initial !important;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  line-height: 1.6;
  word-wrap: break-word;
  padding: 30px;
  font-size: 16px;
  color: #333;
  background-color: #fff;
}
body > *:first-child {
  margin-top: 0 !important;
}
body > *:last-child {
  margin-bottom: 0 !important;
}
body a:not([href]) {
  color: inherit;
  text-decoration: none;
}
body .absent {
  color: #c00;
}
body .anchor {
  position: absolute;
  top: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}
body .anchor:focus {
  outline: none;
}
body h1,
body h2,
body h3,
body h4,
body h5,
body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}
body h1 .octicon-link,
body h2 .octicon-link,
body h3 .octicon-link,
body h4 .octicon-link,
body h5 .octicon-link,
body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}
body h1:hover .anchor,
body h2:hover .anchor,
body h3:hover .anchor,
body h4:hover .anchor,
body h5:hover .anchor,
body h6:hover .anchor {
  padding-left: 8px;
  margin-left: -30px;
  text-decoration: none;
}
body h1:hover .anchor .octicon-link,
body h2:hover .anchor .octicon-link,
body h3:hover .anchor .octicon-link,
body h4:hover .anchor .octicon-link,
body h5:hover .anchor .octicon-link,
body h6:hover .anchor .octicon-link {
  display: inline-block;
}
body h1 tt,
body h2 tt,
body h3 tt,
body h4 tt,
body h5 tt,
body h6 tt,
body h1 code,
body h2 code,
body h3 code,
body h4 code,
body h5 code,
body h6 code {
  font-size: inherit;
}
body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}
body h1 .anchor {
  line-height: 1;
}
body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}
body h2 .anchor {
  line-height: 1;
}
body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}
body h3 .anchor {
  line-height: 1.2;
}
body h4 {
  font-size: 1.25em;
}
body h4 .anchor {
  line-height: 1.2;
}
body h5 {
  font-size: 1em;
}
body h5 .anchor {
  line-height: 1.1;
}
body h6 {
  font-size: 1em;
  color: #777;
}
body h6 .anchor {
  line-height: 1.1;
}
body p,
body blockquote,
body ul,
body ol,
body dl,
body table,
body pre {
  margin-top: 0;
  margin-bottom: 16px;
}
body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}
body ul,
body ol {
  padding-left: 2em;
}
body ul.no-list,
body ol.no-list {
  padding: 0;
  list-style-type: none;
}
body ul ul,
body ul ol,
body ol ol,
body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}
body li > p {
  margin-top: 16px;
}
body dl {
  padding: 0;
}
body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}
body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}
body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}
body blockquote > :first-child {
  margin-top: 0;
}
body blockquote > :last-child {
  margin-bottom: 0;
}
body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}
body table th {
  font-weight: bold;
}
body table th,
body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}
body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}
body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}
body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
body .emoji {
  max-width: none;
}
body span.frame {
  display: block;
  overflow: hidden;
}
body span.frame > span {
  display: block;
  float: left;
  width: auto;
  padding: 7px;
  margin: 13px 0 0;
  overflow: hidden;
  border: 1px solid #ddd;
}
body span.frame span img {
  display: block;
  float: left;
}
body span.frame span span {
  display: block;
  padding: 5px 0 0;
  clear: both;
  color: #333;
}
body span.align-center {
  display: block;
  overflow: hidden;
  clear: both;
}
body span.align-center > span {
  display: block;
  margin: 13px auto 0;
  overflow: hidden;
  text-align: center;
}
body span.align-center span img {
  margin: 0 auto;
  text-align: center;
}
body span.align-right {
  display: block;
  overflow: hidden;
  clear: both;
}
body span.align-right > span {
  display: block;
  margin: 13px 0 0;
  overflow: hidden;
  text-align: right;
}
body span.align-right span img {
  margin: 0;
  text-align: right;
}
body span.float-left {
  display: block;
  float: left;
  margin-right: 13px;
  overflow: hidden;
}
body span.float-left span {
  margin: 13px 0 0;
}
body span.float-right {
  display: block;
  float: right;
  margin-left: 13px;
  overflow: hidden;
}
body span.float-right > span {
  display: block;
  margin: 13px auto 0;
  overflow: hidden;
  text-align: right;
}
body code,
body tt {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0, 0, 0, 0.04);
  border-radius: 3px;
}
body code:before,
body tt:before,
body code:after,
body tt:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}
body code br,
body tt br {
  display: none;
}
body del code {
  text-decoration: inherit;
}
body pre > code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}
body .highlight {
  margin-bottom: 16px;
}
body .highlight pre,
body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}
body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}
body pre {
  word-wrap: normal;
}
body pre code,
body pre tt {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}
body pre code:before,
body pre tt:before,
body pre code:after,
body pre tt:after {
  content: normal;
}
body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}
span.critic.comment > span {
  background-color: #fff;
}
a {
  color: #337ab7;
}

.bracket-matcher .region {
  border-bottom: 1px dotted lime;
  position: absolute;
}
.line-number.bracket-matcher.bracket-matcher {
  color: #abb2bf;
  background-color: #3a3f4b;
}

.spell-check-misspelling .region {
  border-bottom: 2px dotted rgba(255, 51, 51, 0.75);
}
.spell-check-corrections {
  width: 25em !important;
}

pre.editor-colors {
  background-color: #282c34;
  color: #abb2bf;
}
pre.editor-colors .line.cursor-line {
  background-color: rgba(153, 187, 255, 0.04);
}
pre.editor-colors .invisible {
  color: #abb2bf;
}
pre.editor-colors .cursor {
  border-left: 2px solid #528bff;
}
pre.editor-colors .selection .region {
  background-color: #3e4451;
}
pre.editor-colors .bracket-matcher .region {
  border-bottom: 1px solid #528bff;
  box-sizing: border-box;
}
pre.editor-colors .invisible-character {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .indent-guide {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .wrap-guide {
  background-color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .find-result .region.region.region,
pre.editor-colors .current-result .region.region.region {
  border-radius: 2px;
  background-color: rgba(82, 139, 255, 0.24);
  transition: border-color 0.4s;
}
pre.editor-colors .find-result .region.region.region {
  border: 2px solid transparent;
}
pre.editor-colors .current-result .region.region.region {
  border: 2px solid #528bff;
  transition-duration: .1s;
}
pre.editor-colors .gutter .line-number {
  color: #636d83;
  -webkit-font-smoothing: antialiased;
}
pre.editor-colors .gutter .line-number.cursor-line {
  color: #abb2bf;
  background-color: #3a3f4b;
}
pre.editor-colors .gutter .line-number.cursor-line-no-selection {
  background-color: transparent;
}
pre.editor-colors .gutter .line-number .icon-right {
  color: #abb2bf;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed.git-line-removed::before {
  bottom: -3px;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed::after {
  content: "";
  position: absolute;
  left: 0px;
  bottom: 0px;
  width: 25px;
  border-bottom: 1px dotted rgba(224, 82, 82, 0.5);
  pointer-events: none;
}
pre.editor-colors .gutter .line-number.folded,
pre.editor-colors .gutter .line-number:after,
pre.editor-colors .fold-marker:after {
  color: #abb2bf;
}
.syntax--comment {
  color: #5c6370;
  font-style: italic;
}
.syntax--comment .syntax--markup.syntax--link {
  color: #5c6370;
}
.syntax--entity.syntax--name.syntax--type {
  color: #e5c07b;
}
.syntax--entity.syntax--other.syntax--inherited-class {
  color: #e5c07b;
}
.syntax--keyword {
  color: #c678dd;
}
.syntax--keyword.syntax--control {
  color: #c678dd;
}
.syntax--keyword.syntax--operator {
  color: #c678dd;
}
.syntax--keyword.syntax--other.syntax--special-method {
  color: #61afef;
}
.syntax--keyword.syntax--other.syntax--unit {
  color: #d19a66;
}
.syntax--storage {
  color: #c678dd;
}
.syntax--storage.syntax--type.syntax--annotation,
.syntax--storage.syntax--type.syntax--primitive {
  color: #c678dd;
}
.syntax--storage.syntax--modifier.syntax--package,
.syntax--storage.syntax--modifier.syntax--import {
  color: #abb2bf;
}
.syntax--constant {
  color: #d19a66;
}
.syntax--constant.syntax--variable {
  color: #d19a66;
}
.syntax--constant.syntax--character.syntax--escape {
  color: #56b6c2;
}
.syntax--constant.syntax--numeric {
  color: #d19a66;
}
.syntax--constant.syntax--other.syntax--color {
  color: #56b6c2;
}
.syntax--constant.syntax--other.syntax--symbol {
  color: #56b6c2;
}
.syntax--variable {
  color: #e06c75;
}
.syntax--variable.syntax--interpolation {
  color: #be5046;
}
.syntax--variable.syntax--parameter {
  color: #abb2bf;
}
.syntax--string {
  color: #98c379;
}
.syntax--string > .syntax--source,
.syntax--string .syntax--embedded {
  color: #abb2bf;
}
.syntax--string.syntax--regexp {
  color: #56b6c2;
}
.syntax--string.syntax--regexp .syntax--source.syntax--ruby.syntax--embedded {
  color: #e5c07b;
}
.syntax--string.syntax--other.syntax--link {
  color: #e06c75;
}
.syntax--punctuation.syntax--definition.syntax--comment {
  color: #5c6370;
}
.syntax--punctuation.syntax--definition.syntax--method-parameters,
.syntax--punctuation.syntax--definition.syntax--function-parameters,
.syntax--punctuation.syntax--definition.syntax--parameters,
.syntax--punctuation.syntax--definition.syntax--separator,
.syntax--punctuation.syntax--definition.syntax--seperator,
.syntax--punctuation.syntax--definition.syntax--array {
  color: #abb2bf;
}
.syntax--punctuation.syntax--definition.syntax--heading,
.syntax--punctuation.syntax--definition.syntax--identity {
  color: #61afef;
}
.syntax--punctuation.syntax--definition.syntax--bold {
  color: #e5c07b;
  font-weight: bold;
}
.syntax--punctuation.syntax--definition.syntax--italic {
  color: #c678dd;
  font-style: italic;
}
.syntax--punctuation.syntax--section.syntax--embedded {
  color: #be5046;
}
.syntax--punctuation.syntax--section.syntax--method,
.syntax--punctuation.syntax--section.syntax--class,
.syntax--punctuation.syntax--section.syntax--inner-class {
  color: #abb2bf;
}
.syntax--support.syntax--class {
  color: #e5c07b;
}
.syntax--support.syntax--type {
  color: #56b6c2;
}
.syntax--support.syntax--function {
  color: #56b6c2;
}
.syntax--support.syntax--function.syntax--any-method {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--function {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--class,
.syntax--entity.syntax--name.syntax--type.syntax--class {
  color: #e5c07b;
}
.syntax--entity.syntax--name.syntax--section {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--tag {
  color: #e06c75;
}
.syntax--entity.syntax--other.syntax--attribute-name {
  color: #d19a66;
}
.syntax--entity.syntax--other.syntax--attribute-name.syntax--id {
  color: #61afef;
}
.syntax--meta.syntax--class {
  color: #e5c07b;
}
.syntax--meta.syntax--class.syntax--body {
  color: #abb2bf;
}
.syntax--meta.syntax--method-call,
.syntax--meta.syntax--method {
  color: #abb2bf;
}
.syntax--meta.syntax--definition.syntax--variable {
  color: #e06c75;
}
.syntax--meta.syntax--link {
  color: #d19a66;
}
.syntax--meta.syntax--require {
  color: #61afef;
}
.syntax--meta.syntax--selector {
  color: #c678dd;
}
.syntax--meta.syntax--separator {
  color: #abb2bf;
}
.syntax--meta.syntax--tag {
  color: #abb2bf;
}
.syntax--underline {
  text-decoration: underline;
}
.syntax--none {
  color: #abb2bf;
}
.syntax--invalid.syntax--deprecated {
  color: #523d14 !important;
  background-color: #e0c285 !important;
}
.syntax--invalid.syntax--illegal {
  color: white !important;
  background-color: #e05252 !important;
}
.syntax--markup.syntax--bold {
  color: #d19a66;
  font-weight: bold;
}
.syntax--markup.syntax--changed {
  color: #c678dd;
}
.syntax--markup.syntax--deleted {
  color: #e06c75;
}
.syntax--markup.syntax--italic {
  color: #c678dd;
  font-style: italic;
}
.syntax--markup.syntax--heading {
  color: #e06c75;
}
.syntax--markup.syntax--heading .syntax--punctuation.syntax--definition.syntax--heading {
  color: #61afef;
}
.syntax--markup.syntax--link {
  color: #56b6c2;
}
.syntax--markup.syntax--inserted {
  color: #98c379;
}
.syntax--markup.syntax--quote {
  color: #d19a66;
}
.syntax--markup.syntax--raw {
  color: #98c379;
}
.syntax--source.syntax--cs .syntax--keyword.syntax--operator {
  color: #c678dd;
}
.syntax--source.syntax--css .syntax--property-name,
.syntax--source.syntax--css .syntax--property-value {
  color: #828997;
}
.syntax--source.syntax--css .syntax--property-name.syntax--support,
.syntax--source.syntax--css .syntax--property-value.syntax--support {
  color: #abb2bf;
}
.syntax--source.syntax--elixir .syntax--source.syntax--embedded.syntax--source {
  color: #abb2bf;
}
.syntax--source.syntax--elixir .syntax--constant.syntax--language,
.syntax--source.syntax--elixir .syntax--constant.syntax--numeric,
.syntax--source.syntax--elixir .syntax--constant.syntax--definition {
  color: #61afef;
}
.syntax--source.syntax--elixir .syntax--variable.syntax--definition,
.syntax--source.syntax--elixir .syntax--variable.syntax--anonymous {
  color: #c678dd;
}
.syntax--source.syntax--elixir .syntax--parameter.syntax--variable.syntax--function {
  color: #d19a66;
  font-style: italic;
}
.syntax--source.syntax--elixir .syntax--quoted {
  color: #98c379;
}
.syntax--source.syntax--elixir .syntax--keyword.syntax--special-method,
.syntax--source.syntax--elixir .syntax--embedded.syntax--section,
.syntax--source.syntax--elixir .syntax--embedded.syntax--source.syntax--empty {
  color: #e06c75;
}
.syntax--source.syntax--elixir .syntax--readwrite.syntax--module .syntax--punctuation {
  color: #e06c75;
}
.syntax--source.syntax--elixir .syntax--regexp.syntax--section,
.syntax--source.syntax--elixir .syntax--regexp.syntax--string {
  color: #be5046;
}
.syntax--source.syntax--elixir .syntax--separator,
.syntax--source.syntax--elixir .syntax--keyword.syntax--operator {
  color: #d19a66;
}
.syntax--source.syntax--elixir .syntax--variable.syntax--constant {
  color: #e5c07b;
}
.syntax--source.syntax--elixir .syntax--array,
.syntax--source.syntax--elixir .syntax--scope,
.syntax--source.syntax--elixir .syntax--section {
  color: #828997;
}
.syntax--source.syntax--gfm .syntax--markup {
  -webkit-font-smoothing: auto;
}
.syntax--source.syntax--gfm .syntax--link .syntax--entity {
  color: #61afef;
}
.syntax--source.syntax--ini .syntax--keyword.syntax--other.syntax--definition.syntax--ini {
  color: #e06c75;
}
.syntax--source.syntax--java .syntax--storage.syntax--modifier.syntax--import {
  color: #e5c07b;
}
.syntax--source.syntax--java .syntax--storage.syntax--type {
  color: #e5c07b;
}
.syntax--source.syntax--java .syntax--keyword.syntax--operator.syntax--instanceof {
  color: #c678dd;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair {
  color: #e06c75;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair > .syntax--punctuation {
  color: #abb2bf;
}
.syntax--source.syntax--ts .syntax--keyword.syntax--operator {
  color: #56b6c2;
}
.syntax--source.syntax--flow .syntax--keyword.syntax--operator {
  color: #56b6c2;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json {
  color: #e06c75;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation.syntax--string {
  color: #e06c75;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation {
  color: #98c379;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--constant.syntax--language.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--constant.syntax--language.syntax--json {
  color: #56b6c2;
}
.syntax--ng.syntax--interpolation {
  color: #e06c75;
}
.syntax--ng.syntax--interpolation.syntax--begin,
.syntax--ng.syntax--interpolation.syntax--end {
  color: #61afef;
}
.syntax--ng.syntax--interpolation .syntax--function {
  color: #e06c75;
}
.syntax--ng.syntax--interpolation .syntax--function.syntax--begin,
.syntax--ng.syntax--interpolation .syntax--function.syntax--end {
  color: #61afef;
}
.syntax--ng.syntax--interpolation .syntax--bool {
  color: #d19a66;
}
.syntax--ng.syntax--interpolation .syntax--bracket {
  color: #abb2bf;
}
.syntax--ng.syntax--pipe,
.syntax--ng.syntax--operator {
  color: #abb2bf;
}
.syntax--ng.syntax--tag {
  color: #56b6c2;
}
.syntax--ng.syntax--attribute-with-value .syntax--attribute-name {
  color: #e5c07b;
}
.syntax--ng.syntax--attribute-with-value .syntax--string {
  color: #c678dd;
}
.syntax--ng.syntax--attribute-with-value .syntax--string.syntax--begin,
.syntax--ng.syntax--attribute-with-value .syntax--string.syntax--end {
  color: #abb2bf;
}
.syntax--source.syntax--php .syntax--class.syntax--bracket {
  color: #abb2bf;
}
/*
   This defines styling rules for syntax classes.

   See the naming conventions for a list of syntax classes:
   https://flight-manual.atom.io/hacking-atom/sections/syntax-naming-conventions

   When styling rules conflict:
   - The last rule overrides previous rules.
   - The rule with most classes and pseudo-classes overrides the last rule.
*/
.syntax--keyword {
  color: #c678dd;
}
.syntax--keyword.syntax--type {
  color: #56b6c2;
}
.syntax--keyword.syntax--function {
  color: #e06c75;
}
.syntax--keyword.syntax--variable {
  color: #e06c75;
}
.syntax--entity {
  color: #abb2bf;
}
.syntax--entity.syntax--parameter {
  color: #abb2bf;
}
.syntax--entity.syntax--support {
  color: #e06c75;
}
.syntax--entity.syntax--decorator:last-child {
  color: #61afef;
}
.syntax--entity.syntax--label {
  text-decoration: underline;
}
.syntax--entity.syntax--function {
  color: #61afef;
}
.syntax--entity.syntax--operator {
  color: #61afef;
}
.syntax--entity.syntax--operator.syntax--symbolic {
  color: #abb2bf;
}
.syntax--entity.syntax--type {
  color: #56b6c2;
}
.syntax--entity.syntax--tag {
  color: #e06c75;
}
.syntax--entity.syntax--attribute {
  color: #d19a66;
}
.syntax--punctuation {
  color: #abb2bf;
}
.syntax--punctuation.syntax--accessor {
  color: #abb2bf;
}
.syntax--punctuation.syntax--accessor.syntax--member,
.syntax--punctuation.syntax--accessor.syntax--scope {
  color: #c678dd;
}
.syntax--punctuation.syntax--embedded {
  color: #c678dd;
}
.syntax--string {
  color: #98c379;
}
.syntax--string.syntax--immutable {
  color: #98c379;
}
.syntax--string.syntax--part {
  color: #56b6c2;
}
.syntax--string.syntax--interpolation {
  color: #c678dd;
}
.syntax--string.syntax--regexp {
  color: #98c379;
}
.syntax--string.syntax--regexp.syntax--language {
  color: #c678dd;
}
.syntax--string.syntax--regexp.syntax--variable {
  color: #61afef;
}
.syntax--string.syntax--regexp.syntax--punctuation {
  color: #c678dd;
}
.syntax--constant {
  color: #d19a66;
}
.syntax--constant.syntax--character {
  color: #98c379;
}
.syntax--constant.syntax--character.syntax--escape {
  color: #98c379;
}
.syntax--constant.syntax--character.syntax--code {
  color: #56b6c2;
}
.syntax--text {
  color: #abb2bf;
}
.syntax--markup.syntax--heading {
  color: #e06c75;
}
.syntax--markup.syntax--list.syntax--punctuation {
  color: #e06c75;
}
.syntax--markup.syntax--quote {
  color: #5c6370;
  font-style: italic;
}
.syntax--markup.syntax--bold {
  color: #d19a66;
  font-weight: bold;
}
.syntax--markup.syntax--italic {
  color: #c678dd;
  font-style: italic;
}
.syntax--markup.syntax--underline {
  color: #56b6c2;
  text-decoration: underline;
}
.syntax--markup.syntax--strike {
  color: #e06c75;
}
.syntax--markup.syntax--raw {
  color: #98c379;
}
.syntax--markup.syntax--link {
  color: #56b6c2;
}
.syntax--markup.syntax--alt {
  color: #61afef;
}
.syntax--markup.syntax--inserted {
  color: #98c379;
}
.syntax--markup.syntax--inserted .syntax--punctuation {
  color: #98c379;
}
.syntax--markup.syntax--highlighted {
  color: #98c379;
}
.syntax--markup.syntax--highlighted .syntax--punctuation {
  color: #98c379;
}
.syntax--markup.syntax--deleted {
  color: #e06c75;
}
.syntax--markup.syntax--deleted .syntax--punctuation {
  color: #e06c75;
}
.syntax--markup.syntax--changed {
  color: #c678dd;
}
.syntax--markup.syntax--changed .syntax--punctuation {
  color: #c678dd;
}
.syntax--markup.syntax--commented {
  color: #5c6370;
}
.syntax--markup.syntax--commented .syntax--punctuation {
  color: #5c6370;
}
.syntax--comment {
  color: #5c6370;
  font-style: italic;
}
.syntax--comment.syntax--caption {
  color: #6a7181;
  font-weight: bold;
}
.syntax--comment.syntax--term {
  color: #707989;
}
.syntax--comment.syntax--punctuation {
  color: #5c6370;
  font-weight: normal;
}
.syntax--invalid:not(.syntax--punctuation).syntax--illegal {
  color: white !important;
  background-color: #e05252 !important;
}
.syntax--invalid:not(.syntax--punctuation).syntax--deprecated {
  color: #523d14 !important;
  background-color: #e0c285 !important;
}
.syntax--source.syntax--css .syntax--entity.syntax--function {
  color: #828997;
}
.syntax--source.syntax--css .syntax--entity.syntax--function.syntax--support {
  color: #56b6c2;
}
.syntax--source.syntax--css .syntax--entity.syntax--selector {
  color: #d19a66;
}
.syntax--source.syntax--css .syntax--entity.syntax--selector.syntax--tag {
  color: #e06c75;
}
.syntax--source.syntax--css .syntax--entity.syntax--selector.syntax--id {
  color: #61afef;
}
.syntax--source.syntax--css .syntax--entity.syntax--property {
  color: #828997;
}
.syntax--source.syntax--css .syntax--entity.syntax--property.syntax--support {
  color: #abb2bf;
}
.syntax--source.syntax--css .syntax--entity.syntax--variable {
  color: #e06c75;
}
.syntax--source.syntax--css .syntax--constant {
  color: #828997;
}
.syntax--source.syntax--css .syntax--constant.syntax--support {
  color: #abb2bf;
}
.syntax--source.syntax--css .syntax--constant.syntax--numeric {
  color: #d19a66;
}
.syntax--source.syntax--css .syntax--constant.syntax--media {
  color: #d19a66;
}
.syntax--source.syntax--css .syntax--constant.syntax--color {
  color: #d19a66;
}
.syntax--source.syntax--css .syntax--constant.syntax--offset {
  color: #abb2bf;
}
.syntax--source.syntax--css .syntax--constant.syntax--attribute-value {
  color: #98c379;
}
.syntax--source.syntax--css .syntax--punctuation.syntax--selector {
  color: #d19a66;
}
.syntax--source.syntax--css .syntax--punctuation.syntax--selector.syntax--wildcard {
  color: #e06c75;
}
.syntax--source.syntax--css .syntax--punctuation.syntax--selector.syntax--id {
  color: #61afef;
}
.syntax--source.syntax--css .syntax--punctuation.syntax--selector.syntax--attribute {
  color: #abb2bf;
}

/*
 * Your Stylesheet
 *
 * This stylesheet is loaded when Atom starts up and is reloaded automatically
 * when it is changed and saved.
 *
 * Add your own CSS or Less to fully customize Atom.
 * If you are unfamiliar with Less, you can read more about it here:
 * http://lesscss.org
 */
/*
 * Examples
 * (To see them, uncomment and save)
 */
</style>

  </head>
  <body>
    <p><a href="./index.html">HOME</a></p>
<p><a href="https://arxiv.org/a/miyazaki_t_1.html">Preprints</a></p>
<h1>Journal</h1>
<ol reversed>
<li>
<p>Shuya Sano, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya,  Shinichiro Omachi<br>
“Mackerel Fat Content Estimation using RGB and Depth Images”<br>
<em>IEEE Access</em>, 2021 <a href="https://doi.org/10.1109/ACCESS.2021.3134260"><strong>[paper]</strong></a> <strong>IF: 3.367</strong> (in press)</p>
</li>
<li>
<p>Zhengmi Tang, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Stroke-Based Scene Text Erasing Using Synthetic Data for Training”<br>
<em>IEEE Transactions on Image Processing</em>, vol.30, pp.9306-9320, 2021 <a href="https://doi.org/10.1109/TIP.2021.3125260"><strong>[paper]</strong></a> <strong>IF: 10.856</strong></p>
</li>
<li>
<p>Junpei Masuho, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, <a href="https://researchmap.jp/read0055275">Masako Omachi</a> and Shinichiro Omachi<br>
“A Framework for Estimating Gaze Point Information for Location-Based Services”<br>
<em>IEEE Transactions on Vehicular Technology</em>, vol.70, no.9, pp.8468-8477, 2021 <a href="https://doi.org/10.1109/TVT.2021.3101932"><strong>[paper]</strong></a> <strong>IF: 5.978</strong></p>
</li>
<li>
<p>Sho Ishida, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Graph Neural Networks with Multiple Feature Extraction Paths for Chemical Property Estimation”<br>
<em>Molecules</em>, vol.26, no.11, 3125, 2021  <a href="https://doi.org/10.3390/molecules26113125"><strong>[paper]</strong></a> <strong>IF: 4.411</strong></p>
</li>
<li>
<p>Huy Manh Nguyen, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Multiple Visual-Semantic Embedding for Video Retrieval from Query
Sentence”<br>
<em>Applied Sciences</em>, vol.11, no.7, 3214, 2021 <a href="https://doi.org/10.3390/app11073214"><strong>[paper]</strong></a> <strong>IF: 2.679</strong></p>
</li>
<li>
<p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Text Detection Using Multi-Stage Region Proposal Network Sensitive to Text Scale”<br>
<em>Sensors</em>, vol.21, no.4, 2021 <a href="https://doi.org/10.3390/s21041232"><strong>[paper]</strong></a> <strong>IF: 3.576</strong></p>
</li>
<li>
<p>Antoine Chauvet, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Optical Flow-Based Fast Motion Parameters Estimation for Affine Motion Compensation”<br>
<em>Applied Sciences</em>, vol.10, no.2, 2020 <a href="https://doi.org/10.3390/app10020729"><strong>[paper]</strong></a> <strong>IF: 2.679</strong></p>
</li>
<li>
<p>Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Object-Based Video Coding by Visual Saliency and Temporal Correlation”<br>
<em>IEEE Transactions on Emerging Topics in Computing (TETC)</em>, Volume 8, Issue 1, pp.168-178, 2020 <a href="https://doi.org/10.1109/TETC.2017.2695640"><strong>[paper]</strong></a> <strong>IF: 7.691</strong></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro Omachi, <a href="http://www.m.cs.osakafu-u.ac.jp/~masa/index-e_old.shtml">Masakazu Iwamura</a>, <a href="http://human.ait.kyushu-u.ac.jp/~uchida/index-e.html">Seiichi Uchida</a>, <a href="http://www.m.cs.osakafu-u.ac.jp/~kise/index_e.html">Koichi Kise</a><br>
“Automatic Generation of Typographic Font from a Small Font Subset”<br>
<em>IEEE Computer Graphics and Applications</em>, vol.40, issue 1, pp.99-111, 2020 <a href="https://doi.org/10.1109/MCG.2019.2931431"><strong>[paper]</strong></a> <strong>IF: 2.088</strong></p>
</li>
<li>
<p>Antoine Chauvet, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Fast Image Quality Enhancement for HEVC by Post Filtering using Shallow Neural Networks”<br>
<em>IIEEJ Transactions on Image Electronics and Visual Computing</em>, vol.7, no.1, p.2-12, 2019 <a href="https://www.iieej.org/journal-of-the-society/"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya,  Shinichiro Omachi<br>
“Automatic Mackerel Sorting Machine using Global and Local Features”<br>
<em>IEEE Access</em>, vol. 7, pp. 63767-63777, 2019 <a href="https://doi.org/10.1109/ACCESS.2019.2917554"><strong>[paper]</strong></a> <a href="https://github.com/yoyoyo-yo/glcc-frcn.pytorch"><strong>[code]</strong></a> <strong>IF: 3.367</strong></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Structural Data Recognition with Graph Model Boosting”<br>
<em>IEEE Access</em>, vol.6, pp.63606-63618, 2018 <a href="https://doi.org/10.1109/ACCESS.2018.2876860"><strong>[paper]</strong></a> <strong>IF: 3.367</strong></p>
</li>
<li>
<p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Activity Recognition Using Gazed Text and Viewpoint Information for User Support Systems”<br>
<em>Journal of Sensor and Actuator Networks</em>, Volume 7, Issue 3, p.26, 2018 <a href="https://doi.org/10.3390/jsan7030031"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Airi Kitasato, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Automatic Discrimination between Scomber japonicus and Scomber australasicus by Geometric and Texture Features”<br>
<em>Fishes</em>, Volume 3, Issue 3, p.26, 2018 <a href="https://doi.org/10.3390/fishes3030026"><strong>[paper]</strong></a> <strong>IF: 2.385</strong></p>
</li>
<li>
<p>Yosuke Nozue, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Efficient Coding for Video Including Text Using Image Generation”<br>
<em>Journal of Information Processing</em>, vol.24, no.2, pp.330-338, 2016 <a href="https://doi.org/10.2197/ipsjjip.24.330"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Representative Graph Generation for Graph-Based Character Recognition”<br>
<em>The Journal of the Institute of Image Electronics Engineers of Japan</em>, vol.40, no.3, pp.439-447, 2011 <a href="https://doi.org/10.11371/iieej.40.439"><strong>[paper]</strong></a></p>
</li>
</ol>
<p>Japanese:</p>
<ol reversed>
<li>
<p>菅谷 至寛, 坂井 清士郎, <u><strong>宮崎 智</strong></u>, 大町 真一郎<br>
“環境中文字認識を利用した情報提供アプリケーションのための
ウェアラブルシステムの開発”<br>
画像電子学誌 第48巻 第2号, pp.248-257, 2019</p>
</li>
<li>
<p>川村 思織, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“補助線を用いた情景画像からの高精度文字領域抽出”<br>
画像電子学会誌, vol.45, no.1, pp.62-70, 2016 <a href="https://doi.org/10.11371/iieej.45.62"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>
“重み付き最小共通スーパーグラフを用いたシルエット画像認識”<br>
画像電子学会誌, vol.38, no.5, pp.640-647, 2009 <a href="https://doi.org/10.11371/iieej.38.640"><strong>[paper]</strong></a></p>
</li>
</ol>
<h1>International Conference (Refereed)</h1>
<ol reversed>
<li>
<p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>
“Self Texture Transfer Networks for Low Bitrate Image Compression”<br>
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp.1901-1905, 2021
<a href="https://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Iwai_Self_Texture_Transfer_Networks_for_Low_Bitrate_Image_Compression_CVPRW_2021_paper.html"><strong>[paper]</strong></a></p>
</li>
<li>
<p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>
“Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks”<br>
Proceedings of the 25th International Conference on Pattern Recognition (ICPR 2020), pp.8235-8242, Jaunary 2021.
<a href="https://arxiv.org/abs/2008.10314"><strong>[arxiv]</strong></a>
<a href="https://github.com/iwa-shi/fidelity_controllable_compression"><strong>[code]</strong></a></p>
</li>
<li>
<p><a href="https://iwa-shi.github.io/">Shoma Iwai</a>, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>
“Two-Stage Training for High-Fidelity Image Compression with Generative Adversarial Networks”<br>
International Workshops on Emerging ICT, 2020 (Oral)</p>
</li>
<li>
<p>Xi Huang, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>
“Super Resolution for Multi Frames with 3D Feature Extraction and RNN Prediction”<br>
Proceedings of the 2019 International Symposium on Signal Processing Systems (SSPS 2019), pp.82-86, September 2019. <a href="https://doi.org/10.1145/3364908.3364909"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya,  Shinichiro Omachi<br>
“Mackerel Classification using Global and Local Features”<br>
The 2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA), p.1209-1212, 2018 <a href="https://doi.org/10.1109/ETFA.2018.8502584"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Chisato Sugawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Text Retrieval for Japanese Historical Documents by Image Generation”<br>
The 4th International Workshop on Historical Document Imaging and Processing (HIP), p.19-24, 2017 <a href="https://doi.org/10.1145/3151509.3151512"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Yoshito Nagaoka, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Text Detection by Faster R-CNN with Multiple Region Proposal Network”<br>
International Workshop on Camera Based Document Analysis and Recognition (CBDAR), p.15-20, 2017 <a href="https://doi.org/10.1109/ICDAR.2017.343"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Yoshihiro Sugaya, Kento Takeda, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“A Preliminary Study on Location Estimation without Preparation using Ceiling Signboard”<br>
International Conference on Indoor Positioning and Indoor Navigation (IPIN), 179_WIP, 2017 <a href="http://www.ipin2017.org/ipinpapers/179/179.pdf"><strong>[
paper]</strong></a></p>
</li>
<li>
<p>Ofusa Kenichiro, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Glyph-Based Data Augmentation for Accurate Kanji Character Recognition”<br>
International Conference on Document Analysis and Recognition (ICDAR), p.597-602, 2017 <a href="https://doi.org/10.1109/ICDAR.2017.103"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Tomoya Honto, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Analysis of Floor Map Image in Information Board for Indoor Navigation”<br>
International Conference on Indoor Positioning and Indoor Navigation (IPIN), 144_RP, 2017 <a href="https://doi.org/10.1109/IPIN.2017.8115896"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Kiyoshiro Sakai, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Development of Wearable System for Translation of Japanese Texts in the Environment”<br>
Proceedings of International Workshop on Frontiers of Computer Vison (FCV), OS3-2, 2017</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Graph Model Boosting for Structural Data Recognition”<br>
Proceedings of International conference of Pattern Recognition (ICPR), p.1708-1713, 2016 <a href="http://dx.doi.org/10.1109/ICPR.2016.7899882"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Improvement of Map Matching for Indoor Navigation Exploiting Photo of Information Board”<br>
Proceedings of International Conference on Indoor Positioning and Indoor Navigation (IPIN), p.22-26, 2016 <a href="http://www3.uah.es/ipin2016/usb/app/descargas/219_WIP.pdf"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Antoine Chauvet, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya Shinichiro Omachi<br>
“Adaptive Post Filter for Reducing Block Artifacts in High Efficiency Video Coding”<br>
International Conference on Multimedia Systems and Signal Processing, 2016 <a href="http://dx.doi.org/10.1109/ICMSSP.2016.014"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Indoor Localization by Map Matching Using One Image of Information Board”<br>
Proceedings of The Eighth International Conferences on Pervasive Patterns and Applications (PATTERNS), pp.22-26, 2016<br>
<a href="http://www.iaria.org/conferences2016/AwardsPATTERNS16.html"><strong>[<font color="red">Best Paper Award</font>]</strong></a> <a href="http://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2016_1_40_70039"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Object-based Video Coding for Arbitrary Shape by Visual Saliency and Temporal Correlation”<br>
Proceedings of The Eighth International Conference on Creative Content Technologies (CONTENT), pp.13-16, 2016<br>
<a href="http://www.iaria.org/conferences2016/AwardsCONTENT16.html"><strong>[<font color="red">Best Paper Award</font>]</strong></a> <a href="http://www.thinkmind.org/index.php?view=article&amp;articleid=content_2016_1_30_60053"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Airi Kitasato, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Discrimination of Scomber Japonicus and Scomber Australasicus by Dorsal Fin Length and Fork Length”<br>
Proceedings of The 22nd Korea-Japan joint Workshop on Frontiers of Computer Vision (FCV), pp.338-341, 2016</p>
</li>
<li>
<p>Shuhei Toba, Hirotaka Kudo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Ultra-low Resolution Character Recognition System with Pruning Mutual Subspace Method”<br>
Proceedings of International Conference on Consumer Electronics - Taiwan (ICCE-TW), pp.284-285, 2015 <a href="http://dx.doi.org/10.1109/ICCE-TW.2015.7216900"><strong>[paper]</strong></a></p>
</li>
<li>
<p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Estimation of Gazing Points in Environment Using Eye Tracker and Omnidirectional Camera”<br>
Proceedings of 2015 International Conference on Consumer Electronics - Taiwan (ICCE-TW), pp.47-48, 2015 <a href="http://dx.doi.org/10.1109/ICCE-TW.2015.7217003"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Finding Stroke Parts for Rough Text Detection in Scene Images with Random Forest”<br>
Proceedings of Joint Conference of IWAIT and IFMIA, 2015</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Fast Method for Extracting Representative Graph from Decorative Character Images”<br>
Proceedings of IEEE International Conference on Network Infrastructures and Digital Content (IEEE IC-NIDC), pp.219-223, 2010 <a href="http://dx.doi.org/10.1109/ICNIDC.2010.5657776"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Extraction of Representative Structure of Decorative Character Images”<br>
Proceedings of the Chinese Conference on Pattern Recognition (CCPR) and the First CJK Joint Workshop on Pattern Recognition (CJKPR), vol.2, pp.944-948, 2009 <a href="http://dx.doi.org/10.1109/CCPR.2009.5343952"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Extraction of Structure of Silhouette Images by Weighted Minimum Common Supergraph”<br>
Proceedings of the Second Korea-Japan Joint Workshop on Pattern Recognition (KJPR), pp.57-61, 2007</p>
</li>
</ol>
<h1>International Conference (Non-refereed)</h1>
<ol reversed>
<li>
<p>Kyoko Maeda,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Investigation of Convolutional Neural Network Structure for Low Resolution Character Recognition”<br>
International Workshop on Emerging ICT, 2016</p>
</li>
<li>
<p>Kota Oodaira,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Detection of a Key String from Scene Images Using Saliency”<br>
International Workshop on Emerging ICT, 2016</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Rough Detection of Text in Scene Images by Finding Stroke Parts”<br>
Proceedings of the International Workshop on Electronics and Communications, Oral Session 4, 2014</p>
</li>
<li>
<p>Jian Wang, Hiroya Saito, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Automatic Synthesis of Character Pattern Using Patch Transform”<br>
Proceedings of the International Workshop on Electronics and Communications, Oral Session 1, 2014</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Extracting Representative Graph of Decorative Character Images by Random Method”<br>
Proceedings of the Third Student Organizing International Mini-Conference on Information Electronics Systems, pp.67–68, 2010</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Iterative Extraction of Representative Graph Using
Common Features from Decorative Character Images,”<br>
Proceedings of the Second Student Organizing International Mini-Conference on Information Electronics Systems,
pp.85–86, 2009</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Silhouette Image Recognition”<br>
Proceedings of the Third Korea-Japan Joint Workshop on Pattern Recognition, pp.13–
14, 2008</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Structure Extraction from Silhouette Images by Weighted Minimum Common Supergraph”<br>
Proceedings of the First Student Organizing International Mini-Conference on Information Electronics Systems, pp.75–76, 2008</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Extraction of Structure of Shapes Using Weighted Minimum Common Supergraph”<br>
Proceedings of the China-Korea-Japan Graduates Workshop on Electronic Information, pp.49–50, 2008</p>
</li>
</ol>
<h1>Domestic (Non-refereed)</h1>
<ol reversed>
<li>
<p>本田大智, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>
"“歪みのあるフロアマップ上での慣性センサを用いた位置推定法の検討”<br>
2020年度電気関係学会東北支部連合大会講演論文集, S04, 2020</p>
</li>
<li>
<p>西村 遼平,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“ニューラルネットワークを用いた古典籍画像からの文字検出”<br>
電子情報通信学会総合大会学生ポスターセッション, 2F17, 2019</p>
</li>
<li>
<p>石田 聖,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“グラフ構造に着目した分子特性の認識”<br>
電子情報通信学会総合大会学生ポスターセッション, 2F19, 2019</p>
</li>
<li>
<p>中屋 悠資, 鈴木 海斗,&nbsp;<u><strong>宮崎 智</strong></u>, 大町 真一郎<br>
“部首に注目したDeep Leaningによるくずし字の認識を用いた日本古典籍の解析”<br>
電子情報通信学会総合大会学生ポスターセッション, XXX, 2019</p>
</li>
<li>
<p>Nguyen Manh Huy, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, and Shinichiro Omachi<br>
“Joint Video and Language Understanding with Visual-Semantic Embedding”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1H10, 2019<br>
<a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/awards2019.pdf"><strong>[<font color="red">The Best Paper Prize</font>]</strong></a></p>
</li>
<li>
<p>増保純平, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 大町真一郎<br>
“ウェアラブルセンサと固定カメラを用いたユーザの注目位置推定”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1F15, 2019</p>
</li>
<li>
<p>小泉翔太, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>
“略地図画像からの道路領域抽出手法の検討”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1F12, 2019</p>
</li>
<li>
<p>竹村貴文, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>
“屋内撮影画像中の案内板までの実距離推定手法の検討”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1F11, 2019</p>
</li>
<li>
<p>桑野拓朗, 菅谷至寛, <u><strong>宮崎 智</strong></u>, 大町真一郎<br>
“画像生成を取り入れた適応的符号化に関する検討”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1F08, 2019</p>
</li>
<li>
<p>長崎 大, <u><strong>宮崎 智</strong></u>, 菅谷至寛, 大町真一郎<br>
“超解像ネットワークを利用した画像符号化手法の検討”<br>
2019年度電気関係学会東北支部連合大会講演論文集, 1F07, 2019<br>
<a href="https://www.ieice.org/tohoku/award/images/2019awards.pdf"><strong>[<font color="red">学生優秀論文賞</font>]</strong></a></p>
</li>
<li>
<p>黄 希, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“Multi-Frame Super Resolution Using 3D Convolution and RNN Prediction”<br>
平成30年度電気関係学会東北支部連合大会講演論文集, 2F12, 2018<br>
<a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/awards2018.pdf"><strong>[<font color="red">The Encouragement Prize</font>]</strong></a></p>
</li>
<li>
<p>桑野 拓朗, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“超解像を用いた動画像符号化に関する検討”<br>
平成30年度電気関係学会東北支部連合大会講演論文集, 1D14, 2018</p>
</li>
<li>
<p>増保 純平, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“監視カメラを活用したユーザの実世界位置の推定”<br>
平成30年度電気関係学会東北支部連合大会講演論文集, 1I05, 2018</p>
</li>
<li>
<p>竹村 貴文, 菅谷 至寛, <u><strong>宮崎 智</strong></u>, 大町 真一郎<br>
“天吊り案内板を用いた屋内ナビゲーション手法の検討”<br>
平成30年度電気関係学会東北支部連合大会講演論文集, 2A07, 2018</p>
</li>
<li>
<p>佐藤 大亮, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“Determining Important Objects in Scene Image Using Neural Networks”<br>
平成29年度電気関係学会東北支部連合大会講演論文集, 2B12, 2017<br>
<a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2017.html"><strong>[<font color="red">The Encouragement Prize</font>]</strong></a></p>
</li>
<li>
<p>八重樫 日菜子,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“スパースコーディングを用いた動画像符号化に関する検討”<br>
電子情報通信学会総合大会学生ポスターセッション, ISS-SP-200, 2017</p>
</li>
<li>
<p>大平 康太,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“重要度を考慮した情景画像中における文字情報抽出”<br>
電子情報通信学会総合大会学生ポスターセッション, ISS-SP-199, 2017</p>
</li>
<li>
<p>坂井 清士郎,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“文字認識を利用した環境中の日本語英字翻訳を行うウェアラブルシステムの開発”<br>
平成28年度電気関係学会東北支部連合大会講演論文集, 2G04, 2016</p>
</li>
<li>
<p>Kyoko Maeda,&nbsp;<u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Low Resolution Character Recognition Using Convolutional Neural Networks”<br>
平成28年度電気関係学会東北支部連合大会講演論文集, 1A04, 2016</p>
</li>
<li>
<p>Kenta Takeda,&nbsp;Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Signboard Extraction and Recognition in Subway Station Premises”<br>
平成28年度電気関係学会東北支部連合大会講演論文集, 1A03, 2016</p>
</li>
<li>
<p>井上 慶祐,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“スパースコーディングを用いたテキストを含む画像符号化に関する検討”<br>
電子情報通信学会技術研究報告, IE2016-36, vol.116, no.119, pp.5-10, 2016</p>
</li>
<li>
<p>景山 竣, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“パーツの生成による少数サンプルからのフォント生成”<br>
画像の認識・理解シンポジウム (MIRU), PS3-08, 2016</p>
</li>
<li>
<p>大平 康太, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“顕著性を利用した情景画像からの重要な文字列の検出”<br>
画像の認識・理解シンポジウム (MIRU), PS3-42, 2016</p>
</li>
<li>
<p>Toshiaki Sakai, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Graph Learning with Quadratic Programming in Consideration of Class Diversity”<br>
画像の認識・理解シンポジウム (MIRU), PS2-42, 2016</p>
</li>
<li>
<p>酒井 利晃, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“非線形最小化によるグラフのモデルの構築と画像認識”<br>
電子情報通信学会2016年総合大会講演論文集, D-12-96, 2016</p>
</li>
<li>
<p>Shuto Shinbo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Accuracy Improvement of Character Recognition Using Generated Samples by Morphing”<br>
平成27年度電気関係学会東北支部連合大会講演論文集, 2A12, 2015<br>
<a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2015.html"><strong>[<font color="red">The Best Paper Prize</font>]</strong></a></p>
</li>
<li>
<p>Shun Chiba, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Gaze Detection in Omnidirectional Scene by Iterative Image Matching”<br>
平成27年度電気関係学会東北支部連合大会講演論文集, 2A11, 2015</p>
</li>
<li>
<p>Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Survey of Guide Plates and Fundamental Study of Map Image Analysis for Indoor Navigation”<br>
平成27年度電気関係学会東北支部連合大会講演論文集, 1E05, 2015</p>
</li>
<li>
<p>小笠原 和也, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“顕著性マップとGrabCutによる注目物体抽出を用いた動画像符号化”<br>
平成27年度電気関係学会東北支部連合大会講演論文集, 2D10, 2015</p>
</li>
<li>
<p>千葉 駿, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“アイトラッカと全方位カメラを用いた環境中の視点位置推定”<br>
電子情報通信学会技術研究報告, PRMU2014-134, vol.114, no.454, pp.101-102, 2015</p>
</li>
<li>
<p>大島 康嗣, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“ストローク幅特徴を用いた情景画像中の文字検出”<br>
電子情報通信学会技術研究報告, PRMU2014-133, vol.114, no.454, pp.99-100, 2015</p>
</li>
<li>
<p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“画像抽象化による分割圧縮効果改善の検討”<br>
電子情報通信学会技術研究報告, PRMU2014-132, vol.114, no.454, pp.97-98, 2015</p>
</li>
<li>
<p>鳥羽 修平, 工藤 裕貴, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“動画像を用いた超低解像度文字認識”<br>
電子情報通信学会技術研究報告, PRMU2014-131, vol.114, no.454, pp.95-96, 2015</p>
</li>
<li>
<p>吉田 大樹, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“電子基板の欠陥検査のための文字認識”<br>
平成27年度電気関係学会東北支部連合大会講演論文集, 1E06, 2015</p>
</li>
<li>
<p>川村 思織, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“補助線を利用した文字領域抽出”<br>
電子情報通信学会技術研究報告, PRMU2013-152, vol.113, no.431, pp.119-120, 2014</p>
</li>
<li>
<p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“オブジェクトベースの符号化のための画像抽象化を用いた分割圧縮”<br>
画像符号化シンポジウム予稿集, P-4-11, 2014</p>
</li>
<li>
<p>野末 洋佑, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“文字パラメータを利用した動画像中の文字の高効率符号化”<br>
画像符号化シンポジウム予稿集, P-2-11, 2014</p>
</li>
<li>
<p>Shuhei Toba, Hirotaka Kudo, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Ultra-Low Resolution Character Recognition with Increased Training Data and Image Enhancement”<br>
平成26年度電気関係学会東北支部連合大会講演論文集, 2A18, 2014</p>
</li>
<li>
<p>Tatsunori Tsuchiya, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Automatic Generation of Kanji Fonts from Sample Designs”<br>
平成26年度電気関係学会東北支部連合大会講演論文集, 2A17, 2014</p>
</li>
<li>
<p>Yosuke Nozue, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“A Video Coding Method for Scene Text”<br>
平成26年度電気関係学会東北支部連合大会講演論文集, 2A19, 2014</p>
</li>
<li>
<p>石森 亮輔, <u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“抽象化を用いた画像圧縮のための領域分割”<br>
平成26年度電気関係学会東北支部連合大会講演論文集, 2H14, 2014</p>
</li>
<li>
<p>大島 康嗣,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“全方位情景画像のための2段階文字列検出”<br>
画像の認識・理解シンポジウム (MIRU), SS1-42, 2014</p>
</li>
<li>
<p>鳥羽 修平, 工藤 裕貴,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“ぼけ除去及び複数フレームを利用した超低解像度文字認識”<br>
画像の認識・理解シンポジウム (MIRU), SS2-50, 2014</p>
</li>
<li>
<p>土屋 達徳,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“サンプルデザインからの漢字フォントの自動生成”<br>
画像の認識・理解シンポジウム (MIRU), SS3-17, 2014</p>
</li>
<li>
<p>川村 思織,&nbsp;<u><strong>宮崎 智</strong></u>, 菅谷 至寛, 大町 真一郎<br>
“補助線を利用した文字領域抽出”<br>
画像の認識・理解シンポジウム (MIRU), SS3-45, 2014</p>
</li>
<li>
<p><u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Structure Extraction from Multiple Decorative Character Images”<br>
平成21年度電気関係学会東北支部連合大会講演論文集, 2A-11, 2009<br>
<a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2009.html"><font color="red">The Best Paper Prize</font></a></p>
</li>
<li>
<p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>
“共通拡大グラフによる飾り文字画像群からの文字構造抽出”<br>
画像の認識・理解シンポジウム論文集, IS1-16, pp.506-512, 2009</p>
</li>
<li>
<p><u><strong>宮崎 智</strong></u>, 大町 真一郎, 阿曽 弘具<br>
“重み付き最小共通スーパーグラフによるシルエット画像の構造抽出”<br>
画像の認識・理解シンポジウム論文集, IS5-3, pp.1408-1413, 2008</p>
</li>
</ol>
<h1>Misc</h1>
<ol reversed>
<li>
<p>北本 朝展, カラーヌワット タリン, <u><strong>宮崎 智</strong></u>, 山本　和明<br>
“文字データの分析――機械学習によるくずし字認識の可能性とそのインパクト――”<br>
電子情報通信学会会誌, Vol.102 No.6pp.563-568, 2019 <a href="https://www.journal.ieice.org/summary.php?id=k102_6_563&amp;year=2019&amp;lang=J"><strong>[paper]</strong></a></p>
</li>
<li>
<p><u><strong>宮崎 智</strong></u>, 川村 思織, 菅谷 至寛, 大町 真一郎<br>
“ユーザ入力の補助線による情景画像からの高精度文字抽出”,<br>
画像ラボ, 2017年10月号</p>
</li>
</ol>
<h1>Exhibition</h1>
<ol reversed>
<li>全方位文字認識技術（全方位に存在する文字を高精度かつリアルタイムに認識するデモシス
テム）の出展, 最先端IT・エレクトロニクス総合展（CEATEC JAPAN）, 2014 年10 月
7–8 日</li>
<li>コンピュータと人が融和する文字・文書メディアの利用技術（超低解像度文字を高精度に認
識するデモシステム）の出展, 東北大学イノベーションフェア, 2014 年1 月28 日</li>
<li>人間調和型の文字検出および認識手法（人間が見た文字を検出し認識するデモシステム）の
出展, 東北大学電気・情報東京フォーラム, 2013 年11 月25 日</li>
</ol>
<h1>Awards</h1>
<ol reversed>
<li>
<p>石田實記念財団研究奨励賞, 2021 <a href="./imgs/ishida_2021.pdf"><strong>[賞状]</strong></a></p>
</li>
<li>
<p>The Eighth International Conferences on Pervasive Patterns and Applications, Best Paper Award (March 22, 2016)<br>
Kento Tonosaki, Yoshihiro Sugaya, <u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi<br>
“Indoor Localization by Map Matching Using One Image of Information Board”<br>
Proceedings of The Eighth International Conferences on Pervasive Patterns and Applications (PATTERNS), pp.22-26, 2016 <a href="http://www.iaria.org/conferences2016/AwardsPATTERNS16.html"><strong>[URL]</strong></a></p>
</li>
<li>
<p>The Eighth International Conferences on Creative Content Technologies, Best Paper Award (March 22, 2016)<br>
Kazuya Ogasawara, <u><strong>Tomo Miyazaki</strong></u>, Yoshihiro Sugaya, Shinichiro Omachi<br>
“Object-based Video Coding for Arbitrary Shape by Visual Saliency and Temporal Correlation”<br>
Proceedings of The Eighth International Conference on Creative Content Technologies (CONTENT), pp.13-16, 2016 <a href="http://www.iaria.org/conferences2016/AwardsCONTENT16.html"><strong>[URL]</strong></a></p>
</li>
<li>
<p>IEEE Sendai Section, The Best Paper Prize (December 7, 2009)<br>
<u><strong>Tomo Miyazaki</strong></u>, Shinichiro Omachi, Hirotomo Aso<br>
“Structure Extraction from Multiple Decorative Character Images”<br>
Tohoku-Section Joint Convention Record of Institutes of Electrical and Information Engineers, 2A-11, 2009 <a href="http://www.ecei.tohoku.ac.jp/ieee-sendai/event/2009.html"><strong>[URL]</strong></a></p>
</li>
</ol>
<h1>Patent</h1>
<ol reversed>
<li>
<p>大町真一郎, 長岡禎人, 宮崎 智, 菅谷至寛, 山下龍麿, 菅原道晴, 小野寺政行<br>
“魚選別装置、魚選別方法、魚種類推定装置、及び、魚種類推定方法”<br>
特願2018-018601</p>
</li>
<li>
<p>菅谷 至寛, 外崎 健人, 大町 真一郎,&nbsp;宮崎 智,<br>
“情報端末、位置推定方法、および位置推定プログラム”<br>
特願2016-056139，特開2017-173001</p>
</li>
</ol>

  </body>
</html>
